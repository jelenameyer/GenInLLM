{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfc545a9",
   "metadata": {},
   "source": [
    "# Data Wrangling\n",
    "Starting point:\n",
    "- 848 datasets (18 tasks per model (minus the NON_IDEAL_OUTPUTS)) with all logprobs for all answer alternatives of each subtask for all ~1.500 participants. \n",
    "\n",
    "What does this script do\n",
    "- Read all survey data sets\n",
    "- normalise log-prob scores so that they are probabilities that sum up to 1 over all answer options\n",
    "- filter out the probability that the LLm assigned to the answer option which the participant actually chose (for each item)\n",
    "- flip back answers where scale was flipped to cope with prompt sensitivity issues\n",
    "- weigh probabiliies with human answers and divide by all probabilities for that item, to have one number (in the realm if not exact the number of the answer alternatives) per item per model\n",
    "- second weighing strategy, where score of model per item is only weighed with top n most likely probabilities that model assigned\n",
    "- for some scales: add subcategories (each item belongs to a subcategory)\n",
    "- for some tasks, but unsure, reverse back some scores that are asked on a reverse scale due to the nature of the task\n",
    "- concat the itemwise scores for each task per model all together in one `all_data` final data frame and save it.\n",
    "\n",
    "Specials:\n",
    "- for every scale I compared the frey materials quest_raw.csv and quest_proc.csv and tried to trace back how they got from one to the other. Then I did the same transformations\n",
    "- therefore, for AUDIT and FTND and some more the scores are mapped onto a different scale (point system depending on given answer) on the `human_number` level (before mapped to LLM assigned probabilities)\n",
    "- ! for other scales the scores might have to be transfered into another point system as well!\n",
    "- ! SSSV must be reversed in some items, I think, but resluts are awful when done!?!?!?!\n",
    "\n",
    "Goal:\n",
    "- first have one value per item per model\n",
    "- then transform those values in \"outcomes\" for each subscale (like Frey did)\n",
    "- Have 36 values per model! (one per (sub-) scale)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2ebc19",
   "metadata": {},
   "source": [
    "## Packages & Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fef727d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import load_dataframes, filter_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69d790d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----------------------------------------------------------------------------------------------\n",
    "# # Pro Modell × Item die Zähler und Nenner berechnen\n",
    "# # ----------------------------------------------------------------------------------------------\n",
    "# # - Zähler = Summe von (Antwort * Wahrscheinlichkeit)\n",
    "# # - Nenner = Summe von (Wahrscheinlichkeiten)\n",
    "\n",
    "# def compute_weighted_score(group):\n",
    "#     numerator = (group[\"human_number\"] * group[\"prob_pred\"]).sum()\n",
    "#     denominator = group[\"prob_pred\"].sum()\n",
    "#     return numerator / denominator if denominator > 0 else None\n",
    "\n",
    "\n",
    "# # Funktion für Top-n gewichteten Score\n",
    "# def compute_top_n_weighted_score(group, n = 100):\n",
    "#     # Sortiere die Zeilen nach Wahrscheinlichkeit absteigend\n",
    "#     top_n = group.sort_values(\"prob_pred\", ascending=False).head(n)\n",
    "#     # Numerator = Summe von (Antwort * Wahrscheinlichkeit)\n",
    "#     numerator = (top_n[\"human_number\"] * top_n[\"prob_pred\"]).sum()\n",
    "#     # Denominator = Summe von Wahrscheinlichkeiten der Top n\n",
    "#     denominator = top_n[\"prob_pred\"].sum()\n",
    "#     return numerator / denominator if denominator > 0 else None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # produce df with one value per model per item --------------------------------------------------\n",
    "# # more compact version (that runs faster)\n",
    "# def get_LLM_value_per_item(data):\n",
    "#     grouped = data.groupby([\"experiment\", \"model\", \"item\"])\n",
    "#     score = (grouped[\"human_number\"].apply(lambda x: (x * data.loc[x.index, \"prob_pred\"]).sum())\n",
    "#              / grouped[\"prob_pred\"].sum())\n",
    "#     return score.reset_index(name=\"score\")\n",
    "\n",
    "# # produce df with one value per model per item for top n version --------------------------------------------------\n",
    "# def get_LLM_value_per_item_top_n(data):\n",
    "#     new_df = (\n",
    "#     data.groupby([\"experiment\", \"model\", \"item\"])[[\"human_number\", \"prob_pred\"]]\n",
    "#       .apply(compute_top_n_weighted_score)\n",
    "#       .reset_index(name=\"score_top_n\")\n",
    "#     )\n",
    "#     return(new_df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04d4fc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------\n",
    "# Weighted score per model × item\n",
    "# ----------------------------------------------------------------------------------------------\n",
    "\n",
    "def compute_weighted_score(group):\n",
    "    numerator = (group[\"human_number\"] * group[\"prob_pred\"]).sum()\n",
    "    denominator = group[\"prob_pred\"].sum()\n",
    "    return numerator / denominator if denominator > 0 else None\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------\n",
    "# Top-n weighted score\n",
    "# ----------------------------------------------------------------------------------------------s\n",
    "\n",
    "def compute_top_n_weighted_score(group, n=100):\n",
    "    # Sort rows by probability descending\n",
    "    top_n = group.sort_values(\"prob_pred\", ascending=False).head(n)\n",
    "\n",
    "    numerator = (top_n[\"human_number\"] * top_n[\"prob_pred\"]).sum()\n",
    "    denominator = top_n[\"prob_pred\"].sum()\n",
    "\n",
    "    return numerator / denominator if denominator > 0 else None\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------\n",
    "# One value per model × item (safe version, no indexing bug)\n",
    "# ----------------------------------------------------------------------------------------------\n",
    "\n",
    "def get_LLM_value_per_item(data):\n",
    "    grouped = data.groupby([\"experiment\", \"model\", \"item\"])\n",
    "    \n",
    "    score = grouped.apply(\n",
    "        lambda g: (g[\"human_number\"] * g[\"prob_pred\"]).sum() / g[\"prob_pred\"].sum()\n",
    "    )\n",
    "    \n",
    "    return score.reset_index(name=\"score\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------\n",
    "# One value per model × item (Top-n version, safe)\n",
    "# ----------------------------------------------------------------------------------------------\n",
    "\n",
    "def get_LLM_value_per_item_top_n(data, n=100):\n",
    "    result = (\n",
    "        data.groupby([\"experiment\", \"model\", \"item\"])[[\"human_number\", \"prob_pred\"]]\n",
    "            .apply(lambda g: compute_top_n_weighted_score(g, n=n))\n",
    "            .reset_index(name=\"score_top_n\")\n",
    "    )\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eece353e",
   "metadata": {},
   "source": [
    "## AUDIT SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65a51faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrame shape: (712264, 11)\n",
      "Total models: 46\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "AUDIT_data = load_dataframes(task_name=\"AUDIT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd081b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise answer option sum to one (tun so als hätten wir sehr guten Prompt, dann würde LLM nur zwischen möglichen Antwortalternativen aussuchen, da simulieren wir dadurch)\n",
    "mask = (AUDIT_data[\"item\"] == 1)\n",
    "AUDIT_data.loc[mask, \"prob_1\"] = np.exp(AUDIT_data.loc[mask, \"1\"])/(np.exp(AUDIT_data.loc[mask, \"1\"]) + np.exp(AUDIT_data.loc[mask, \"2\"]))\n",
    "AUDIT_data.loc[mask, \"prob_2\"] = np.exp(AUDIT_data.loc[mask, \"2\"])/(np.exp(AUDIT_data.loc[mask, \"1\"]) + np.exp(AUDIT_data.loc[mask, \"2\"]))\n",
    "\n",
    "mask = (AUDIT_data[\"item\"].isin([10, 11]))\n",
    "AUDIT_data.loc[mask, \"prob_1\"] = np.exp(AUDIT_data.loc[mask, \"1\"])/(np.exp(AUDIT_data.loc[mask, \"1\"]) + np.exp(AUDIT_data.loc[mask, \"2\"]) + np.exp(AUDIT_data.loc[mask, \"3\"]))\n",
    "AUDIT_data.loc[mask, \"prob_2\"] = np.exp(AUDIT_data.loc[mask, \"2\"])/(np.exp(AUDIT_data.loc[mask, \"1\"]) + np.exp(AUDIT_data.loc[mask, \"2\"]) + np.exp(AUDIT_data.loc[mask, \"3\"]))\n",
    "AUDIT_data.loc[mask, \"prob_3\"] = np.exp(AUDIT_data.loc[mask, \"3\"])/(np.exp(AUDIT_data.loc[mask, \"1\"]) + np.exp(AUDIT_data.loc[mask, \"2\"]) + np.exp(AUDIT_data.loc[mask, \"3\"]))\n",
    "\n",
    "\n",
    "mask = (AUDIT_data[\"item\"].isin([2, 3, 4, 5, 6, 7, 8, 9]))\n",
    "AUDIT_data.loc[mask, \"prob_1\"] = np.exp(AUDIT_data.loc[mask, \"1\"])/(np.exp(AUDIT_data.loc[mask, \"1\"]) + np.exp(AUDIT_data.loc[mask, \"2\"]) + np.exp(AUDIT_data.loc[mask, \"3\"]) + np.exp(AUDIT_data.loc[mask, \"4\"]) + np.exp(AUDIT_data.loc[mask, \"5\"]))\n",
    "AUDIT_data.loc[mask, \"prob_2\"] = np.exp(AUDIT_data.loc[mask, \"2\"])/(np.exp(AUDIT_data.loc[mask, \"1\"]) + np.exp(AUDIT_data.loc[mask, \"2\"]) + np.exp(AUDIT_data.loc[mask, \"3\"]) + np.exp(AUDIT_data.loc[mask, \"4\"]) + np.exp(AUDIT_data.loc[mask, \"5\"]))\n",
    "AUDIT_data.loc[mask, \"prob_3\"] = np.exp(AUDIT_data.loc[mask, \"3\"])/(np.exp(AUDIT_data.loc[mask, \"1\"]) + np.exp(AUDIT_data.loc[mask, \"2\"]) + np.exp(AUDIT_data.loc[mask, \"3\"]) + np.exp(AUDIT_data.loc[mask, \"4\"]) + np.exp(AUDIT_data.loc[mask, \"5\"]))\n",
    "AUDIT_data.loc[mask, \"prob_4\"] = np.exp(AUDIT_data.loc[mask, \"4\"])/(np.exp(AUDIT_data.loc[mask, \"1\"]) + np.exp(AUDIT_data.loc[mask, \"2\"]) + np.exp(AUDIT_data.loc[mask, \"3\"]) + np.exp(AUDIT_data.loc[mask, \"4\"]) + np.exp(AUDIT_data.loc[mask, \"5\"]))\n",
    "AUDIT_data.loc[mask, \"prob_5\"] = np.exp(AUDIT_data.loc[mask, \"5\"])/(np.exp(AUDIT_data.loc[mask, \"1\"]) + np.exp(AUDIT_data.loc[mask, \"2\"]) + np.exp(AUDIT_data.loc[mask, \"3\"]) + np.exp(AUDIT_data.loc[mask, \"4\"]) + np.exp(AUDIT_data.loc[mask, \"5\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b52a6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out probability LLM assigned to real item answer \n",
    "AUDIT_data=filter_pred_prob(AUDIT_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1421f934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flip back human answers where they were flipped\n",
    "mask = (AUDIT_data[\"flipped\"] == \"yes\") & (AUDIT_data[\"item\"] == 1)\n",
    "AUDIT_data.loc[mask, \"human_number\"] = 3 - AUDIT_data.loc[mask, \"human_number\"]\n",
    "mask = (AUDIT_data[\"flipped\"] == \"yes\") & (AUDIT_data[\"item\"].isin([10, 11]))\n",
    "AUDIT_data.loc[mask, \"human_number\"] = 4 - AUDIT_data.loc[mask, \"human_number\"]\n",
    "mask = (AUDIT_data[\"flipped\"] == \"yes\") & (AUDIT_data[\"item\"].isin([2, 3, 4, 5, 6, 7, 8, 9]))\n",
    "AUDIT_data.loc[mask, \"human_number\"] = 6 - AUDIT_data.loc[mask, \"human_number\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a81387b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mappings for each AUDIT question:\n",
    "audit_maps = {\n",
    "    1: {1: 1, 2: 0},                            # AlcSplit\n",
    "    2: {1: 0, 2: 1, 3: 2, 4: 3, 5: 4},          # AUDIT_1 (in proc data Frey)\n",
    "    3: {1: 0, 2: 1, 3: 2, 4: 3, 5: 4},          # AUDIT_2 (in proc data Frey)\n",
    "    4: {1: 0, 2: 1, 3: 2, 4: 3, 5: 4},          # AUDIT_3 (in proc data Frey)\n",
    "    5: {1: 0, 2: 1, 3: 2, 4: 3, 5: 4},          # AUDIT_4 (in proc data Frey)\n",
    "    6: {1: 0, 2: 1, 3: 2, 4: 3, 5: 4},          # AUDIT_5 (in proc data Frey)\n",
    "    7: {1: 0, 2: 1, 3: 2, 4: 3, 5: 4},          # AUDIT_6 (in proc data Frey)\n",
    "    8: {1: 0, 2: 1, 3: 2, 4: 3, 5: 4},          # AUDIT_7 (in proc data Frey)\n",
    "    9: {1: 0, 2: 1, 3: 2, 4: 3, 5: 4},          # AUDIT_8 (in proc data Frey)\n",
    "    10: {1: 0, 2: 2, 3: 4},                     # AUDIT_10 (in proc data Frey)\n",
    "    11: {1: 0, 2: 2, 3: 4},                     # AUDIT_11 (in proc data Frey)\n",
    "\n",
    "}\n",
    "\n",
    "# Apply mapping row-wise based on item number\n",
    "def recode_audit(row):\n",
    "    mapping = audit_maps.get(row[\"item\"])\n",
    "    if mapping is not None:\n",
    "        return mapping.get(row[\"human_number\"], None)  # None if invalid code\n",
    "    return row[\"human_number\"]  \n",
    "\n",
    "AUDIT_data[\"human_number\"] = AUDIT_data.apply(recode_audit, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0d05126",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g6/6tx7kz_51_92m63qx0q2c2lw0000gn/T/ipykernel_4710/2881964319.py:32: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  score = grouped.apply(\n"
     ]
    }
   ],
   "source": [
    "# produce df with one value per model per item \n",
    "model_item_scores_AUDIT = get_LLM_value_per_item(AUDIT_data)\n",
    "model_item_scores_AUDIT_top_n = get_LLM_value_per_item_top_n(AUDIT_data)\n",
    "\n",
    "# Merge them on the grouping keys\n",
    "model_item_scores_AUDIT = model_item_scores_AUDIT.merge(\n",
    "    model_item_scores_AUDIT_top_n,\n",
    "    on=[\"experiment\", \"model\", \"item\"],\n",
    "    how=\"inner\" \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d65688",
   "metadata": {},
   "source": [
    "## BARRAT SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc730c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrame shape: (2082420, 10)\n",
      "Total models: 46\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "BARRAT_data = load_dataframes(task_name=\"BARRAT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b88e1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise answer option sum to one\n",
    "BARRAT_data[\"prob_1\"] = np.exp(BARRAT_data[\"1\"])/(np.exp(BARRAT_data[\"1\"]) + np.exp(BARRAT_data[\"2\"]) + np.exp(BARRAT_data[\"3\"]) + np.exp(BARRAT_data[\"4\"]))\n",
    "BARRAT_data[\"prob_2\"] = np.exp(BARRAT_data[\"2\"])/(np.exp(BARRAT_data[\"1\"]) + np.exp(BARRAT_data[\"2\"]) + np.exp(BARRAT_data[\"3\"]) + np.exp(BARRAT_data[\"4\"]))\n",
    "BARRAT_data[\"prob_3\"] = np.exp(BARRAT_data[\"3\"])/(np.exp(BARRAT_data[\"1\"]) + np.exp(BARRAT_data[\"2\"]) + np.exp(BARRAT_data[\"3\"]) + np.exp(BARRAT_data[\"4\"]))\n",
    "BARRAT_data[\"prob_4\"] = np.exp(BARRAT_data[\"4\"])/(np.exp(BARRAT_data[\"1\"]) + np.exp(BARRAT_data[\"2\"]) + np.exp(BARRAT_data[\"3\"]) + np.exp(BARRAT_data[\"4\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9945c979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out probability LLM assigned to real item answer \n",
    "BARRAT_data=filter_pred_prob(BARRAT_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29d02886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flip back human answers where they were flipped\n",
    "mask = (BARRAT_data[\"flipped\"] == True)\n",
    "BARRAT_data.loc[mask, \"human_number\"] = 5 - BARRAT_data.loc[mask, \"human_number\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d415b71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse human answers (again) where the items where reversed phrased\n",
    "\n",
    "# add whether item was reverse coded\n",
    "reverse_coded = {\n",
    "    1: True, 2: False, 3: False,  4: False, 5: False,  6: False,  7: True,  8: True,  9: True,  10: True,\n",
    "    11: False, 12: True, 13: True,  14: False, 15: True,  16: False,  17: False,  18: False,  19: False,  20: True,\n",
    "    21: False, 22: False, 23: False,  24: False, 25: False,  26: False,  27: False,  28: False,  29: True,  30: True\n",
    "    }\n",
    "\n",
    "# Apply mapping row-wise based on item number\n",
    "BARRAT_data[\"reverse_coded\"] = BARRAT_data[\"item\"].map(reverse_coded)\n",
    "\n",
    "\n",
    "# flip back answers that where reverse coded\n",
    "mask = (BARRAT_data[\"reverse_coded\"] == True)\n",
    "BARRAT_data.loc[mask, \"human_number\"] = 5 - BARRAT_data.loc[mask, \"human_number\"]\n",
    "# drop reverse-coded column (not needed in final data)\n",
    "BARRAT_data = BARRAT_data.drop(columns=[\"reverse_coded\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3100efef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g6/6tx7kz_51_92m63qx0q2c2lw0000gn/T/ipykernel_4710/2881964319.py:32: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  score = grouped.apply(\n"
     ]
    }
   ],
   "source": [
    "# produce df with one value per model per item \n",
    "model_item_scores_BARRAT = get_LLM_value_per_item(BARRAT_data)\n",
    "model_item_scores_BARRAT_top_n = get_LLM_value_per_item_top_n(BARRAT_data)\n",
    "\n",
    "# Merge them on the grouping keys\n",
    "model_item_scores_BARRAT = model_item_scores_BARRAT.merge(\n",
    "    model_item_scores_BARRAT_top_n,\n",
    "    on=[\"experiment\", \"model\", \"item\"],\n",
    "    how=\"inner\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38eda3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding task specific categories to save in all data\n",
    "\n",
    "# add item categories\n",
    "item_to_category = {\n",
    "    1: \"BISn\", 2: \"BISm\", 3: \"BISm\",  4: \"BISm\", 5: \"BISa\",  6: \"BISa\",  7: \"BISn\",  8: \"BISn\",  9: \"BISa\",  10: \"BISn\",\n",
    "    11: \"BISa\", 12: \"BISn\", 13: \"BISn\",  14: \"BISn\", 15: \"BISn\",  16: \"BISm\",  17: \"BISm\",  18: \"BISn\",  19: \"BISm\",  20: \"BISa\",\n",
    "    21: \"BISm\", 22: \"BISm\", 23: \"BISm\",  24: \"BISa\", 25: \"BISm\",  26: \"BISa\",  27: \"BISn\",  28: \"BISa\",  29: \"BISn\",  30: \"BISm\"\n",
    "}\n",
    "\n",
    "model_item_scores_BARRAT[\"category\"] = model_item_scores_BARRAT[\"item\"].map(item_to_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ddb9972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dfs\n",
    "all_data = pd.concat([model_item_scores_AUDIT, model_item_scores_BARRAT], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0226ac",
   "metadata": {},
   "source": [
    "## CARE TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1491661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrame shape: (1320614, 106)\n",
      "Total models: 46\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "CARE_data = load_dataframes(task_name=\"CARE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d59c8fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get probabilities out of log-probabilities\n",
    "\n",
    "cols = [str(i) for i in range(0, 100)]\n",
    "# Compute normalized probabilities\n",
    "exp_vals = np.exp(CARE_data[cols])\n",
    "prob_vals = exp_vals.div(exp_vals.sum(axis=1), axis=0)\n",
    "\n",
    "# Rename columns all at once\n",
    "prob_vals.columns = [f\"prob_{i}\" for i in range(0, 100)]\n",
    "\n",
    "# Join to original dataframe in one step\n",
    "CARE_data = pd.concat([CARE_data, prob_vals], axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a389bebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out probability LLM assigned to real item answer \n",
    "CARE_data=filter_pred_prob(CARE_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "914d1030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g6/6tx7kz_51_92m63qx0q2c2lw0000gn/T/ipykernel_4710/2881964319.py:32: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  score = grouped.apply(\n"
     ]
    }
   ],
   "source": [
    "# produce df with one value per model per item \n",
    "model_item_scores_CARE = get_LLM_value_per_item(CARE_data)\n",
    "model_item_scores_CARE_top_n = get_LLM_value_per_item_top_n(CARE_data)\n",
    "\n",
    "# Merge them on the grouping keys\n",
    "model_item_scores_CARE = model_item_scores_CARE.merge(\n",
    "    model_item_scores_CARE_top_n,\n",
    "    on=[\"experiment\", \"model\", \"item\"],\n",
    "    how=\"inner\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a344c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding task specific categories to save in all data\n",
    "# add item categories\n",
    "item_to_category = {\n",
    "    1: \"CAREa\", 2: \"CAREa\", 3: \"CAREa\",  4: \"CAREa\", 5: \"CAREa\",  6: \"CAREa\",  7: \"CAREa\",  8: \"CAREa\",  9: \"CAREa\",  10: \"CAREs\",\n",
    "    11: \"CAREs\", 12: \"CAREs\", 13: \"CAREs\",  14: \"CAREs\", 15: \"CAREs\",  16: \"CAREw\",  17: \"CAREw\",  18: \"CAREw\",  19: \"CAREw\"\n",
    "}\n",
    "\n",
    "model_item_scores_CARE[\"category\"] = model_item_scores_CARE[\"item\"].map(item_to_category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8720af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([all_data, model_item_scores_CARE], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1623f7ae",
   "metadata": {},
   "source": [
    "## DAST SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1e720e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrame shape: (1391040, 8)\n",
      "Total models: 46\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "DAST_data = load_dataframes(task_name=\"DAST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "acf31561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise answer option sum to one \n",
    "DAST_data[\"prob_1\"] = np.exp(DAST_data[\"1\"])/(np.exp(DAST_data[\"1\"]) + np.exp(DAST_data[\"2\"]))\n",
    "DAST_data[\"prob_2\"] = np.exp(DAST_data[\"2\"])/(np.exp(DAST_data[\"1\"]) + np.exp(DAST_data[\"2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6833d7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out probability LLM assigned to real item answer \n",
    "DAST_data=filter_pred_prob(DAST_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d9b749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flip back human answers where they were flipped\n",
    "mask = (DAST_data[\"flipped\"] == True) \n",
    "DAST_data.loc[mask, \"human_number\"] = 3 - DAST_data.loc[mask, \"human_number\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79db3eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mappings for each DAST question:\n",
    "dast_maps = {\n",
    "    1: {1: 1, 2: 0},                           \n",
    "    2: {1: 1, 2: 0},\n",
    "    3: {1: 1, 2: 0},\n",
    "    4: {1: 0, 2: 1},\n",
    "    5: {1: 0, 2: 1},\n",
    "    6: {1: 1, 2: 0},\n",
    "    7: {1: 1, 2: 0},\n",
    "    8: {1: 1, 2: 0},\n",
    "    9: {1: 1, 2: 0},\n",
    "    10: {1: 1, 2: 0},\n",
    "    11: {1: 1, 2: 0},\n",
    "    12: {1: 1, 2: 0},\n",
    "    13: {1: 1, 2: 0},\n",
    "    14: {1: 1, 2: 0},\n",
    "    15: {1: 1, 2: 0},\n",
    "    16: {1: 1, 2: 0},\n",
    "    17: {1: 1, 2: 0},\n",
    "    18: {1: 1, 2: 0},\n",
    "    19: {1: 1, 2: 0},\n",
    "    20: {1: 1, 2: 0}\n",
    "}\n",
    "\n",
    "# Apply mapping row-wise based on item number\n",
    "def recode_dast(row):\n",
    "    mapping = dast_maps.get(row[\"item\"])\n",
    "    if mapping is not None:\n",
    "        return mapping.get(row[\"human_number\"], None)  # None if invalid code\n",
    "    return row[\"human_number\"]  \n",
    "\n",
    "DAST_data[\"human_number\"] = DAST_data.apply(recode_dast, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "806cd5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g6/6tx7kz_51_92m63qx0q2c2lw0000gn/T/ipykernel_4710/2881964319.py:32: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  score = grouped.apply(\n"
     ]
    }
   ],
   "source": [
    "# produce df with one value per model per item \n",
    "model_item_scores_DAST = get_LLM_value_per_item(DAST_data)\n",
    "model_item_scores_DAST_top_n = get_LLM_value_per_item_top_n(DAST_data)\n",
    "\n",
    "# Merge them on the grouping keys\n",
    "model_item_scores_DAST = model_item_scores_DAST.merge(\n",
    "    model_item_scores_DAST_top_n,\n",
    "    on=[\"experiment\", \"model\", \"item\"],\n",
    "    how=\"inner\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54525c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dfs\n",
    "all_data = pd.concat([all_data, model_item_scores_DAST], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a376a9e9",
   "metadata": {},
   "source": [
    "## DM SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a50dd425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrame shape: (1318866, 10)\n",
      "Total models: 46\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "DM_data = load_dataframes(task_name=\"DM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8a1b71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise answer option sum to one\n",
    "DM_data[\"prob_1\"] = np.exp(DM_data[\"1\"])/(np.exp(DM_data[\"1\"]) + np.exp(DM_data[\"2\"]) + np.exp(DM_data[\"3\"]) + np.exp(DM_data[\"4\"]))\n",
    "DM_data[\"prob_2\"] = np.exp(DM_data[\"2\"])/(np.exp(DM_data[\"1\"]) + np.exp(DM_data[\"2\"]) + np.exp(DM_data[\"3\"]) + np.exp(DM_data[\"4\"]))\n",
    "DM_data[\"prob_3\"] = np.exp(DM_data[\"3\"])/(np.exp(DM_data[\"1\"]) + np.exp(DM_data[\"2\"]) + np.exp(DM_data[\"3\"]) + np.exp(DM_data[\"4\"]))\n",
    "DM_data[\"prob_4\"] = np.exp(DM_data[\"4\"])/(np.exp(DM_data[\"1\"]) + np.exp(DM_data[\"2\"]) + np.exp(DM_data[\"3\"]) + np.exp(DM_data[\"4\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b778d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out probability LLM assigned to real item answer \n",
    "DM_data=filter_pred_prob(DM_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22d0cc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flip back human answers where they were flipped\n",
    "mask = (DM_data[\"flipped\"] == True) \n",
    "DM_data.loc[mask, \"human_number\"] = 5 - DM_data.loc[mask, \"human_number\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f0d4d183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mappings for DM, so that all 4s are transformed to 1s (like in original Fey dataset):\n",
    "# hier Abweichung von sonst Orientierung an Frey quest_proc df, aber da sonst später Umwandlung, hier gleich zu Scale 0-2\n",
    "mapping = {\n",
    "    4: 1,\n",
    "    3: 2,\n",
    "    2: 1,\n",
    "    1: 0\n",
    "} # here, I already go one step further tha human process, directly to real scores that are then summed/averaged for final sum\n",
    "DM_data[\"human_number\"] = DM_data[\"human_number\"].map(mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb8e201f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g6/6tx7kz_51_92m63qx0q2c2lw0000gn/T/ipykernel_4710/2881964319.py:32: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  score = grouped.apply(\n"
     ]
    }
   ],
   "source": [
    "# produce df with one value per model per item \n",
    "model_item_scores_DM = get_LLM_value_per_item(DM_data)\n",
    "model_item_scores_DM_top_n = get_LLM_value_per_item_top_n(DM_data)\n",
    "\n",
    "# Merge them on the grouping keys\n",
    "model_item_scores_DM = model_item_scores_DM.merge(\n",
    "    model_item_scores_DM_top_n,\n",
    "    on=[\"experiment\", \"model\", \"item\"],\n",
    "    how=\"inner\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "125ae7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dfs\n",
    "all_data = pd.concat([all_data, model_item_scores_DM], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf36e830",
   "metadata": {},
   "source": [
    "## DOSPERT SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4828b1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrame shape: (2780240, 11)\n",
      "Total models: 46\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "DOSPERT_data = load_dataframes(task_name=\"DOSPERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "df6de3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise answer option sum to one\n",
    "DOSPERT_data[\"prob_1\"] = np.exp(DOSPERT_data[\"1\"])/(np.exp(DOSPERT_data[\"1\"]) + np.exp(DOSPERT_data[\"2\"]) + np.exp(DOSPERT_data[\"3\"]) + np.exp(DOSPERT_data[\"4\"]) + np.exp(DOSPERT_data[\"5\"]))\n",
    "DOSPERT_data[\"prob_2\"] = np.exp(DOSPERT_data[\"2\"])/(np.exp(DOSPERT_data[\"1\"]) + np.exp(DOSPERT_data[\"2\"]) + np.exp(DOSPERT_data[\"3\"]) + np.exp(DOSPERT_data[\"4\"]) + np.exp(DOSPERT_data[\"5\"]))\n",
    "DOSPERT_data[\"prob_3\"] = np.exp(DOSPERT_data[\"3\"])/(np.exp(DOSPERT_data[\"1\"]) + np.exp(DOSPERT_data[\"2\"]) + np.exp(DOSPERT_data[\"3\"]) + np.exp(DOSPERT_data[\"4\"]) + np.exp(DOSPERT_data[\"5\"]))\n",
    "DOSPERT_data[\"prob_4\"] = np.exp(DOSPERT_data[\"4\"])/(np.exp(DOSPERT_data[\"1\"]) + np.exp(DOSPERT_data[\"2\"]) + np.exp(DOSPERT_data[\"3\"]) + np.exp(DOSPERT_data[\"4\"]) + np.exp(DOSPERT_data[\"5\"]))\n",
    "DOSPERT_data[\"prob_5\"] = np.exp(DOSPERT_data[\"5\"])/(np.exp(DOSPERT_data[\"1\"]) + np.exp(DOSPERT_data[\"2\"]) + np.exp(DOSPERT_data[\"3\"]) + np.exp(DOSPERT_data[\"4\"]) + np.exp(DOSPERT_data[\"5\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c87c6117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out probability LLM assigned to real item answer \n",
    "DOSPERT_data=filter_pred_prob(DOSPERT_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "270ff6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flip back human answers where they were flipped\n",
    "mask = (DOSPERT_data[\"flipped\"] == 'yes') \n",
    "DOSPERT_data.loc[mask, \"human_number\"] = 6 - DOSPERT_data.loc[mask, \"human_number\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a15c7fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g6/6tx7kz_51_92m63qx0q2c2lw0000gn/T/ipykernel_4710/2881964319.py:32: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  score = grouped.apply(\n"
     ]
    }
   ],
   "source": [
    "# produce df with one value per model per item \n",
    "model_item_scores_DOSPERT = get_LLM_value_per_item(DOSPERT_data)\n",
    "model_item_scores_DOSPERT_top_n = get_LLM_value_per_item_top_n(DOSPERT_data)\n",
    "\n",
    "# Merge them on the grouping keys\n",
    "model_item_scores_DOSPERT = model_item_scores_DOSPERT.merge(\n",
    "    model_item_scores_DOSPERT_top_n,\n",
    "    on=[\"experiment\", \"model\", \"item\"],\n",
    "    how=\"inner\" \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "005799af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding task specific categories to save in all data\n",
    "\n",
    "# add item categories\n",
    "item_to_category = {\n",
    "    1: \"Social\", 10: \"Social\", 16: \"Social\", 19: \"Social\", 23: \"Social\", 26: \"Social\", 34: \"Social\", 35: \"Social\",\n",
    "    2: \"Recreational\", 6: \"Recreational\", 15: \"Recreational\", 17: \"Recreational\", 21: \"Recreational\", 31: \"Recreational\", 37: \"Recreational\", 38: \"Recreational\",\n",
    "    3: \"Gambling\", 11: \"Gambling\", 22: \"Gambling\", 33: \"Gambling\",\n",
    "    4: \"Health\", 8: \"Health\", 27: \"Health\", 29: \"Health\", 32: \"Health\", 36: \"Health\", 39: \"Health\", 40: \"Health\",\n",
    "    5: \"Ethical\", 9: \"Ethical\", 12: \"Ethical\", 13: \"Ethical\", 14: \"Ethical\", 20: \"Ethical\", 25: \"Ethical\", 28: \"Ethical\",\n",
    "    7: \"Investment\", 18: \"Investment\", 24: \"Investment\", 30: \"Investment\"\n",
    "}\n",
    "\n",
    "model_item_scores_DOSPERT[\"category\"] = model_item_scores_DOSPERT[\"item\"].map(item_to_category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eae478ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dfs\n",
    "all_data = pd.concat([all_data, model_item_scores_DOSPERT], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cbad99",
   "metadata": {},
   "source": [
    "## FTND SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b79895fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrame shape: (163162, 10)\n",
      "Total models: 46\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "FTND_data = load_dataframes(task_name=\"FTND\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3d512cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise answer option sum to one (tun so als hätten wir sehr guten Prompt, dann würde LLM nur zwischen möglichen Antwortalternativen aussuchen, da simulieren wir dadurch)\n",
    "mask = (FTND_data[\"item\"] == 1)\n",
    "FTND_data.loc[mask, \"prob_1\"] = np.exp(FTND_data.loc[mask, \"1\"])/(np.exp(FTND_data.loc[mask, \"1\"]) + np.exp(FTND_data.loc[mask, \"2\"]) + np.exp(FTND_data.loc[mask, \"3\"]))\n",
    "FTND_data.loc[mask, \"prob_2\"] = np.exp(FTND_data.loc[mask, \"2\"])/(np.exp(FTND_data.loc[mask, \"1\"]) + np.exp(FTND_data.loc[mask, \"2\"]) + np.exp(FTND_data.loc[mask, \"3\"]))\n",
    "FTND_data.loc[mask, \"prob_3\"] = np.exp(FTND_data.loc[mask, \"3\"])/(np.exp(FTND_data.loc[mask, \"1\"]) + np.exp(FTND_data.loc[mask, \"2\"]) + np.exp(FTND_data.loc[mask, \"3\"]))\n",
    "\n",
    "mask = (FTND_data[\"item\"].isin([3, 4, 6, 7]))\n",
    "FTND_data.loc[mask, \"prob_1\"] = np.exp(FTND_data.loc[mask, \"1\"])/(np.exp(FTND_data.loc[mask, \"1\"]) + np.exp(FTND_data.loc[mask, \"2\"]))\n",
    "FTND_data.loc[mask, \"prob_2\"] = np.exp(FTND_data.loc[mask, \"2\"])/(np.exp(FTND_data.loc[mask, \"1\"]) + np.exp(FTND_data.loc[mask, \"2\"]))\n",
    "\n",
    "mask = (FTND_data[\"item\"].isin([2, 5]))\n",
    "FTND_data.loc[mask, \"prob_1\"] = np.exp(FTND_data.loc[mask, \"1\"])/(np.exp(FTND_data.loc[mask, \"1\"]) + np.exp(FTND_data.loc[mask, \"2\"]) + np.exp(FTND_data.loc[mask, \"3\"]) + np.exp(FTND_data.loc[mask, \"4\"]))\n",
    "FTND_data.loc[mask, \"prob_2\"] = np.exp(FTND_data.loc[mask, \"2\"])/(np.exp(FTND_data.loc[mask, \"1\"]) + np.exp(FTND_data.loc[mask, \"2\"]) + np.exp(FTND_data.loc[mask, \"3\"]) + np.exp(FTND_data.loc[mask, \"4\"]))\n",
    "FTND_data.loc[mask, \"prob_3\"] = np.exp(FTND_data.loc[mask, \"3\"])/(np.exp(FTND_data.loc[mask, \"1\"]) + np.exp(FTND_data.loc[mask, \"2\"]) + np.exp(FTND_data.loc[mask, \"3\"]) + np.exp(FTND_data.loc[mask, \"4\"]))\n",
    "FTND_data.loc[mask, \"prob_4\"] = np.exp(FTND_data.loc[mask, \"4\"])/(np.exp(FTND_data.loc[mask, \"1\"]) + np.exp(FTND_data.loc[mask, \"2\"]) + np.exp(FTND_data.loc[mask, \"3\"]) + np.exp(FTND_data.loc[mask, \"4\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f407d56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out probability LLM assigned to real item answer \n",
    "FTND_data=filter_pred_prob(FTND_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6c3668c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flip back human answers where they were flipped\n",
    "mask = (FTND_data[\"flipped\"] == True) & (FTND_data[\"item\"] == 1)\n",
    "FTND_data.loc[mask, \"human_number\"] = 4 - FTND_data.loc[mask, \"human_number\"]\n",
    "mask = (FTND_data[\"flipped\"] == True) & (FTND_data[\"item\"].isin([3, 4, 6, 7]))\n",
    "FTND_data.loc[mask, \"human_number\"] = 3 - FTND_data.loc[mask, \"human_number\"]\n",
    "mask = (FTND_data[\"flipped\"] == True) & (FTND_data[\"item\"].isin([2, 5]))\n",
    "FTND_data.loc[mask, \"human_number\"] = 5 - FTND_data.loc[mask, \"human_number\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "813faab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mappings for each FTND question:\n",
    "ftnd_maps = {\n",
    "    1: {1: 2, 2: 0, 3: 1},             # FTND_1Eingangsfrage: smoke?\n",
    "    2: {1: 3, 2: 2, 3: 1, 4: 0},       # FTND_1: time until first cigarette\n",
    "    3: {1: 1, 2: 0},                   # FTND_2: difficult to refrain\n",
    "    4: {1: 1, 2: 0},                   # FTND_3: which cigarette hardest to give up\n",
    "    5: {1: 0, 2: 1, 3: 2, 4: 3},       # FTND_4: cigarettes per day\n",
    "    6: {1: 1, 2: 0},                   # FTND_5: smoke more frequently in morning\n",
    "    7: {1: 1, 2: 0}                    # FTND_6: smoke when ill\n",
    "}\n",
    "\n",
    "# Apply mapping row-wise based on item number\n",
    "def recode_ftnd(row):\n",
    "    mapping = ftnd_maps.get(row[\"item\"])\n",
    "    if mapping is not None:\n",
    "        return mapping.get(row[\"human_number\"], None) \n",
    "    return row[\"human_number\"]  \n",
    "\n",
    "FTND_data[\"human_number\"] = FTND_data.apply(recode_ftnd, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0bf07e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g6/6tx7kz_51_92m63qx0q2c2lw0000gn/T/ipykernel_4710/2881964319.py:32: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  score = grouped.apply(\n"
     ]
    }
   ],
   "source": [
    "# produce df with one value per model per item \n",
    "model_item_scores_FTND = get_LLM_value_per_item(FTND_data)\n",
    "model_item_scores_FTND_top_n = get_LLM_value_per_item_top_n(FTND_data)\n",
    "\n",
    "# Merge them on the grouping keys\n",
    "model_item_scores_FTND = model_item_scores_FTND.merge(\n",
    "    model_item_scores_FTND_top_n,\n",
    "    on=[\"experiment\", \"model\", \"item\"],\n",
    "    how=\"inner\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ce78b07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dfs\n",
    "all_data = pd.concat([all_data, model_item_scores_FTND], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54306e48",
   "metadata": {},
   "source": [
    "## GABS SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dc20119a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrame shape: (581210, 10)\n",
      "Total models: 46\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "GABS_data = load_dataframes(task_name=\"GABS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8cf18026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise answer option sum to one\n",
    "\n",
    "# columns representing log-probabilities\n",
    "answer_cols = [\"1\", \"2\", \"3\", \"4\"]\n",
    "\n",
    "# make a copy to avoid SettingWithCopy warnings\n",
    "GABS_data = GABS_data.copy()\n",
    "\n",
    "# case 1: item == 1 → only options 1 and 2\n",
    "mask_item1 = GABS_data[\"item\"] == 1\n",
    "exp_vals_item1 = np.exp(GABS_data.loc[mask_item1, [\"1\", \"2\"]])\n",
    "probs_item1 = exp_vals_item1.div(exp_vals_item1.sum(axis=1), axis=0)\n",
    "probs_item1.columns = [\"prob_1\", \"prob_2\"]\n",
    "\n",
    "# case 2: items 2–17 → options 1–4\n",
    "mask_item2plus = GABS_data[\"item\"].between(2, 17)\n",
    "exp_vals_item2plus = np.exp(GABS_data.loc[mask_item2plus, answer_cols])\n",
    "probs_item2plus = exp_vals_item2plus.div(exp_vals_item2plus.sum(axis=1), axis=0)\n",
    "probs_item2plus.columns = [f\"prob_{c}\" for c in answer_cols]\n",
    "\n",
    "# merge both parts back into original df\n",
    "GABS_data = GABS_data.join(pd.concat([probs_item1, probs_item2plus]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "45796a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out probability LLM assigned to real item answer \n",
    "GABS_data=filter_pred_prob(GABS_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "29b40036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flip back human answers where they were flipped\n",
    "mask = (GABS_data[\"flipped\"] == True) & (GABS_data[\"item\"] == 1)\n",
    "GABS_data.loc[mask, \"human_number\"] = 3 - GABS_data.loc[mask, \"human_number\"]\n",
    "mask = (GABS_data[\"flipped\"] == True) & (GABS_data[\"item\"].isin(range(2,17)))\n",
    "GABS_data.loc[mask, \"human_number\"] = 5 - GABS_data.loc[mask, \"human_number\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6cb6a938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g6/6tx7kz_51_92m63qx0q2c2lw0000gn/T/ipykernel_4710/2881964319.py:32: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  score = grouped.apply(\n"
     ]
    }
   ],
   "source": [
    "# produce df with one value per model per item \n",
    "model_item_scores_GABS = get_LLM_value_per_item(GABS_data)\n",
    "model_item_scores_GABS_top_n = get_LLM_value_per_item_top_n(GABS_data)\n",
    "\n",
    "# Merge them on the grouping keys\n",
    "model_item_scores_GABS = model_item_scores_GABS.merge(\n",
    "    model_item_scores_GABS_top_n,\n",
    "    on=[\"experiment\", \"model\", \"item\"],\n",
    "    how=\"inner\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "abcf63ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dfs\n",
    "all_data = pd.concat([all_data, model_item_scores_GABS], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed59198",
   "metadata": {},
   "source": [
    "## PG SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e97aa4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrame shape: (1127322, 13)\n",
      "Total models: 46\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "PG_data = load_dataframes(task_name=\"PG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "66851dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise answer option sum to one \n",
    "\n",
    "mask = (PG_data[\"item\"].isin([1, 26]))\n",
    "PG_data.loc[mask, \"prob_1\"] = np.exp(PG_data.loc[mask, \"1\"])/(np.exp(PG_data.loc[mask, \"1\"]) + np.exp(PG_data.loc[mask, \"2\"]))\n",
    "PG_data.loc[mask, \"prob_2\"] = np.exp(PG_data.loc[mask, \"2\"])/(np.exp(PG_data.loc[mask, \"1\"]) + np.exp(PG_data.loc[mask, \"2\"]))\n",
    "\n",
    "\n",
    "\n",
    "mask = (PG_data[\"item\"].isin(range(2, 21)))\n",
    "PG_data.loc[mask, \"prob_1\"] = np.exp(PG_data.loc[mask, \"1\"])/(np.exp(PG_data.loc[mask, \"1\"]) + np.exp(PG_data.loc[mask, \"2\"]) + np.exp(PG_data.loc[mask, \"3\"]) + np.exp(PG_data.loc[mask, \"4\"]) + np.exp(PG_data.loc[mask, \"5\"]))\n",
    "PG_data.loc[mask, \"prob_2\"] = np.exp(PG_data.loc[mask, \"2\"])/(np.exp(PG_data.loc[mask, \"1\"]) + np.exp(PG_data.loc[mask, \"2\"]) + np.exp(PG_data.loc[mask, \"3\"]) + np.exp(PG_data.loc[mask, \"4\"]) + np.exp(PG_data.loc[mask, \"5\"]))\n",
    "PG_data.loc[mask, \"prob_3\"] = np.exp(PG_data.loc[mask, \"3\"])/(np.exp(PG_data.loc[mask, \"1\"]) + np.exp(PG_data.loc[mask, \"2\"]) + np.exp(PG_data.loc[mask, \"3\"]) + np.exp(PG_data.loc[mask, \"4\"]) + np.exp(PG_data.loc[mask, \"5\"]))\n",
    "PG_data.loc[mask, \"prob_4\"] = np.exp(PG_data.loc[mask, \"4\"])/(np.exp(PG_data.loc[mask, \"1\"]) + np.exp(PG_data.loc[mask, \"2\"]) + np.exp(PG_data.loc[mask, \"3\"]) + np.exp(PG_data.loc[mask, \"4\"]) + np.exp(PG_data.loc[mask, \"5\"]))\n",
    "PG_data.loc[mask, \"prob_5\"] = np.exp(PG_data.loc[mask, \"5\"])/(np.exp(PG_data.loc[mask, \"1\"]) + np.exp(PG_data.loc[mask, \"2\"]) + np.exp(PG_data.loc[mask, \"3\"]) + np.exp(PG_data.loc[mask, \"4\"]) + np.exp(PG_data.loc[mask, \"5\"]))\n",
    "\n",
    "\n",
    "\n",
    "mask = (PG_data[\"item\"] == 25)\n",
    "PG_data.loc[mask, \"prob_1\"] = np.exp(PG_data.loc[mask, \"1\"])/(np.exp(PG_data.loc[mask, \"1\"]) + np.exp(PG_data.loc[mask, \"2\"]) + np.exp(PG_data.loc[mask, \"3\"]) + np.exp(PG_data.loc[mask, \"4\"]) + np.exp(PG_data.loc[mask, \"5\"]) + np.exp(PG_data.loc[mask, \"6\"]))\n",
    "PG_data.loc[mask, \"prob_2\"] = np.exp(PG_data.loc[mask, \"2\"])/(np.exp(PG_data.loc[mask, \"1\"]) + np.exp(PG_data.loc[mask, \"2\"]) + np.exp(PG_data.loc[mask, \"3\"]) + np.exp(PG_data.loc[mask, \"4\"]) + np.exp(PG_data.loc[mask, \"5\"]) + np.exp(PG_data.loc[mask, \"6\"]))\n",
    "PG_data.loc[mask, \"prob_3\"] = np.exp(PG_data.loc[mask, \"3\"])/(np.exp(PG_data.loc[mask, \"1\"]) + np.exp(PG_data.loc[mask, \"2\"]) + np.exp(PG_data.loc[mask, \"3\"]) + np.exp(PG_data.loc[mask, \"4\"]) + np.exp(PG_data.loc[mask, \"5\"]) + np.exp(PG_data.loc[mask, \"6\"]))\n",
    "PG_data.loc[mask, \"prob_4\"] = np.exp(PG_data.loc[mask, \"4\"])/(np.exp(PG_data.loc[mask, \"1\"]) + np.exp(PG_data.loc[mask, \"2\"]) + np.exp(PG_data.loc[mask, \"3\"]) + np.exp(PG_data.loc[mask, \"4\"]) + np.exp(PG_data.loc[mask, \"5\"]) + np.exp(PG_data.loc[mask, \"6\"]))\n",
    "PG_data.loc[mask, \"prob_5\"] = np.exp(PG_data.loc[mask, \"5\"])/(np.exp(PG_data.loc[mask, \"1\"]) + np.exp(PG_data.loc[mask, \"2\"]) + np.exp(PG_data.loc[mask, \"3\"]) + np.exp(PG_data.loc[mask, \"4\"]) + np.exp(PG_data.loc[mask, \"5\"]) + np.exp(PG_data.loc[mask, \"6\"]))\n",
    "PG_data.loc[mask, \"prob_6\"] = np.exp(PG_data.loc[mask, \"6\"])/(np.exp(PG_data.loc[mask, \"1\"]) + np.exp(PG_data.loc[mask, \"2\"]) + np.exp(PG_data.loc[mask, \"3\"]) + np.exp(PG_data.loc[mask, \"4\"]) + np.exp(PG_data.loc[mask, \"5\"]) + np.exp(PG_data.loc[mask, \"6\"]))\n",
    "\n",
    "\n",
    "mask = (PG_data[\"item\"].isin([21, 22, 23, 24, 27, 28, 29, 30, 31, 32]))\n",
    "PG_data.loc[mask, \"prob_0\"] = np.exp(PG_data.loc[mask, \"0\"])/(np.exp(PG_data.loc[mask, \"0\"]) + np.exp(PG_data.loc[mask, \"1\"]))\n",
    "PG_data.loc[mask, \"prob_1\"] = np.exp(PG_data.loc[mask, \"1\"])/(np.exp(PG_data.loc[mask, \"0\"]) + np.exp(PG_data.loc[mask, \"1\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dfc61164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out probability LLM assigned to real item answer \n",
    "PG_data=filter_pred_prob(PG_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "66d9d25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flip back human answers where they were flipped\n",
    "mask = (PG_data[\"flipped\"] == True) & (PG_data[\"item\"].isin([1, 26]))\n",
    "PG_data.loc[mask, \"human_number\"] = 3 - PG_data.loc[mask, \"human_number\"]\n",
    "\n",
    "mask = (PG_data[\"flipped\"] == True) & (PG_data[\"item\"].isin(range(2, 21)))\n",
    "PG_data.loc[mask, \"human_number\"] = 6 - PG_data.loc[mask, \"human_number\"]\n",
    "\n",
    "mask = (PG_data[\"flipped\"] == True) & (PG_data[\"item\"] == 25)\n",
    "PG_data.loc[mask, \"human_number\"] = 7 - PG_data.loc[mask, \"human_number\"]\n",
    "\n",
    "mask = (PG_data[\"flipped\"] == True) & (PG_data[\"item\"].isin([21, 22, 23, 24, 27, 28, 29, 30, 31, 32]))\n",
    "PG_data.loc[mask, \"human_number\"] = 1 - PG_data.loc[mask, \"human_number\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9508ec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mappings for each GABS question:\n",
    "pg_maps = {     \n",
    "    1: {1: 1, 2: 0},                      \n",
    "    2: {1: 4, 2: 3, 3: 2, 4: 1, 5: 0},\n",
    "    3: {1: 4, 2: 3, 3: 2, 4: 1, 5: 0},\n",
    "    4: {1: 4, 2: 3, 3: 2, 4: 1, 5: 0},\n",
    "    5: {1: 4, 2: 3, 3: 2, 4: 1, 5: 0},\n",
    "    6: {1: 4, 2: 3, 3: 2, 4: 1, 5: 0},\n",
    "    7: {1: 4, 2: 3, 3: 2, 4: 1, 5: 0},\n",
    "    8: {1: 4, 2: 3, 3: 2, 4: 1, 5: 0},\n",
    "    9: {1: 4, 2: 3, 3: 2, 4: 1, 5: 0},\n",
    "    10: {1: 4, 2: 3, 3: 2, 4: 1, 5: 0},\n",
    "    11: {1: 4, 2: 3, 3: 2, 4: 1, 5: 0},\n",
    "    12: {1: 4, 2: 3, 3: 2, 4: 1, 5: 0},\n",
    "    13: {1: 4, 2: 3, 3: 2, 4: 1, 5: 0},\n",
    "    14: {1: 4, 2: 3, 3: 2, 4: 1, 5: 0},\n",
    "    15: {1: 4, 2: 3, 3: 2, 4: 1, 5: 0},\n",
    "    16: {1: 4, 2: 3, 3: 2, 4: 1, 5: 0},\n",
    "    17: {1: 4, 2: 3, 3: 2, 4: 1, 5: 0},\n",
    "    18: {1: 4, 2: 3, 3: 2, 4: 1, 5: 0},\n",
    "    19: {1: 4, 2: 3, 3: 2, 4: 1, 5: 0},\n",
    "    20: {1: 4, 2: 3, 3: 2, 4: 1, 5: 0},\n",
    "    26: {1: 1, 2: 0}\n",
    "}\n",
    "\n",
    "# Apply mapping row-wise based on item number\n",
    "def recode_pg(row):\n",
    "    mapping = pg_maps.get(row[\"item\"])\n",
    "    if mapping is not None:\n",
    "        return mapping.get(row[\"human_number\"], None)  # None if invalid code\n",
    "    return row[\"human_number\"]  \n",
    "\n",
    "PG_data[\"human_number\"] = PG_data.apply(recode_pg, axis=1)\n",
    "\n",
    "# jetzt ist es konsistent mit Freys df quest_proc (außer an den Items, wo es in der gleichen Skale bei ESS_GABS_ausserh_01-10 plötzlich ?vergessen? wurde bei Frey)\n",
    "# aber meiner Meinung nach müsste man, damit man die Skala in binned factors umwandeln kann, noch alles in die gleiche Richtung bringen,\n",
    "# 1 und 26 sind in falscher Richtung! -> habe ich jetzt gefixt obwohl Abweichung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c5f8e0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g6/6tx7kz_51_92m63qx0q2c2lw0000gn/T/ipykernel_4710/2881964319.py:32: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  score = grouped.apply(\n"
     ]
    }
   ],
   "source": [
    "# produce df with one value per model per item \n",
    "model_item_scores_PG = get_LLM_value_per_item(PG_data)\n",
    "model_item_scores_PG_top_n = get_LLM_value_per_item_top_n(PG_data)\n",
    "\n",
    "# Merge them on the grouping keys\n",
    "model_item_scores_PG = model_item_scores_PG.merge(\n",
    "    model_item_scores_PG_top_n,\n",
    "    on=[\"experiment\", \"model\", \"item\"],\n",
    "    how=\"inner\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5e0c6aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dfs\n",
    "all_data = pd.concat([all_data, model_item_scores_PG], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b01fef",
   "metadata": {},
   "source": [
    "## PRI SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "80602ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrame shape: (1110624, 13)\n",
      "Total models: 46\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "PRI_data = load_dataframes(task_name=\"PRI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "39ee7b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise answer option sum to one \n",
    "\n",
    "mask = (PRI_data[\"item\"].isin([1, 3, 5, 7, 9, 11, 13, 15]))\n",
    "PRI_data.loc[mask, \"prob_1\"] = np.exp(PRI_data.loc[mask, \"1\"])/(np.exp(PRI_data.loc[mask, \"1\"]) + np.exp(PRI_data.loc[mask, \"2\"]))\n",
    "PRI_data.loc[mask, \"prob_2\"] = np.exp(PRI_data.loc[mask, \"2\"])/(np.exp(PRI_data.loc[mask, \"1\"]) + np.exp(PRI_data.loc[mask, \"2\"]))\n",
    "\n",
    "\n",
    "\n",
    "mask = (PRI_data[\"item\"].isin([2, 4, 6, 8, 10, 12, 14, 16]))\n",
    "PRI_data.loc[mask, \"prob_1\"] = np.exp(PRI_data.loc[mask, \"1\"])/(np.exp(PRI_data.loc[mask, \"1\"]) + np.exp(PRI_data.loc[mask, \"2\"]) + np.exp(PRI_data.loc[mask, \"3\"]) + np.exp(PRI_data.loc[mask, \"4\"]) + np.exp(PRI_data.loc[mask, \"5\"]) + np.exp(PRI_data.loc[mask, \"6\"]) + np.exp(PRI_data.loc[mask, \"7\"]))\n",
    "PRI_data.loc[mask, \"prob_2\"] = np.exp(PRI_data.loc[mask, \"2\"])/(np.exp(PRI_data.loc[mask, \"1\"]) + np.exp(PRI_data.loc[mask, \"2\"]) + np.exp(PRI_data.loc[mask, \"3\"]) + np.exp(PRI_data.loc[mask, \"4\"]) + np.exp(PRI_data.loc[mask, \"5\"]) + np.exp(PRI_data.loc[mask, \"6\"]) + np.exp(PRI_data.loc[mask, \"7\"]))\n",
    "PRI_data.loc[mask, \"prob_3\"] = np.exp(PRI_data.loc[mask, \"3\"])/(np.exp(PRI_data.loc[mask, \"1\"]) + np.exp(PRI_data.loc[mask, \"2\"]) + np.exp(PRI_data.loc[mask, \"3\"]) + np.exp(PRI_data.loc[mask, \"4\"]) + np.exp(PRI_data.loc[mask, \"5\"]) + np.exp(PRI_data.loc[mask, \"6\"]) + np.exp(PRI_data.loc[mask, \"7\"]))\n",
    "PRI_data.loc[mask, \"prob_4\"] = np.exp(PRI_data.loc[mask, \"4\"])/(np.exp(PRI_data.loc[mask, \"1\"]) + np.exp(PRI_data.loc[mask, \"2\"]) + np.exp(PRI_data.loc[mask, \"3\"]) + np.exp(PRI_data.loc[mask, \"4\"]) + np.exp(PRI_data.loc[mask, \"5\"]) + np.exp(PRI_data.loc[mask, \"6\"]) + np.exp(PRI_data.loc[mask, \"7\"]))\n",
    "PRI_data.loc[mask, \"prob_5\"] = np.exp(PRI_data.loc[mask, \"5\"])/(np.exp(PRI_data.loc[mask, \"1\"]) + np.exp(PRI_data.loc[mask, \"2\"]) + np.exp(PRI_data.loc[mask, \"3\"]) + np.exp(PRI_data.loc[mask, \"4\"]) + np.exp(PRI_data.loc[mask, \"5\"]) + np.exp(PRI_data.loc[mask, \"6\"]) + np.exp(PRI_data.loc[mask, \"7\"]))\n",
    "PRI_data.loc[mask, \"prob_6\"] = np.exp(PRI_data.loc[mask, \"6\"])/(np.exp(PRI_data.loc[mask, \"1\"]) + np.exp(PRI_data.loc[mask, \"2\"]) + np.exp(PRI_data.loc[mask, \"3\"]) + np.exp(PRI_data.loc[mask, \"4\"]) + np.exp(PRI_data.loc[mask, \"5\"]) + np.exp(PRI_data.loc[mask, \"6\"]) + np.exp(PRI_data.loc[mask, \"7\"]))\n",
    "PRI_data.loc[mask, \"prob_7\"] = np.exp(PRI_data.loc[mask, \"7\"])/(np.exp(PRI_data.loc[mask, \"1\"]) + np.exp(PRI_data.loc[mask, \"2\"]) + np.exp(PRI_data.loc[mask, \"3\"]) + np.exp(PRI_data.loc[mask, \"4\"]) + np.exp(PRI_data.loc[mask, \"5\"]) + np.exp(PRI_data.loc[mask, \"6\"]) + np.exp(PRI_data.loc[mask, \"7\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fe6591f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out probability LLM assigned to real item answer \n",
    "PRI_data=filter_pred_prob(PRI_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0ff04f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flip back human answers where they were flipped\n",
    "mask = (PRI_data[\"flipped\"] == True) & (PRI_data[\"item\"].isin([1, 3, 5, 7, 9, 11, 13, 15]))\n",
    "PRI_data.loc[mask, \"human_number\"] = 3 - PRI_data.loc[mask, \"human_number\"]\n",
    "\n",
    "mask = (PRI_data[\"flipped\"] == True) & (PRI_data[\"item\"].isin([2, 4, 6, 8, 10, 12, 14, 16]))\n",
    "PRI_data.loc[mask, \"human_number\"] = 8 - PRI_data.loc[mask, \"human_number\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7ae95ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g6/6tx7kz_51_92m63qx0q2c2lw0000gn/T/ipykernel_4710/2881964319.py:32: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  score = grouped.apply(\n"
     ]
    }
   ],
   "source": [
    "# produce df with one value per model per item \n",
    "model_item_scores_PRI = get_LLM_value_per_item(PRI_data)\n",
    "model_item_scores_PRI_top_n = get_LLM_value_per_item_top_n(PRI_data)\n",
    "\n",
    "# Merge them on the grouping keys\n",
    "model_item_scores_PRI = model_item_scores_PRI.merge(\n",
    "    model_item_scores_PRI_top_n,\n",
    "    on=[\"experiment\", \"model\", \"item\"],\n",
    "    how=\"inner\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ffab96f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding task specific categories to save in all data\n",
    "\n",
    "# add item categories\n",
    "item_to_category = {\n",
    "     1: \"decision\", 3: \"decision\", 5: \"decision\", 7: \"decision\", 9: \"decision\", 11: \"decision\", 13: \"decision\", 15: \"decision\",\n",
    "     2: \"certainty\", 4: \"certainty\", 6: \"certainty\", 8: \"certainty\", 10: \"certainty\", 12: \"certainty\", 14: \"certainty\", 16: \"certainty\"\n",
    "}\n",
    "\n",
    "model_item_scores_PRI[\"category\"] = model_item_scores_PRI[\"item\"].map(item_to_category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "68281602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dfs\n",
    "all_data = pd.concat([all_data, model_item_scores_PRI], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d1f8ec",
   "metadata": {},
   "source": [
    "## SOEP SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2db32d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrame shape: (486542, 17)\n",
      "Total models: 46\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "SOEP_data = load_dataframes(task_name=\"SOEP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "59969196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get probabilities out of log-probabilities\n",
    "\n",
    "cols = [str(i) for i in range(1, 12)]\n",
    "# Compute normalized probabilities\n",
    "exp_vals = np.exp(SOEP_data[cols])\n",
    "prob_vals = exp_vals.div(exp_vals.sum(axis=1), axis=0)\n",
    "\n",
    "# Rename columns all at once\n",
    "prob_vals.columns = [f\"prob_{i}\" for i in range(1, 12)]\n",
    "\n",
    "# Join to original dataframe in one step\n",
    "SOEP_data = pd.concat([SOEP_data, prob_vals], axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "aaea5af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out probability LLM assigned to real item answer \n",
    "SOEP_data=filter_pred_prob(SOEP_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "829772eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flip back human answers where they were flipped\n",
    "mask = (SOEP_data[\"flipped\"] == \"yes\") \n",
    "SOEP_data.loc[mask, \"human_number\"] = 12 - SOEP_data.loc[mask, \"human_number\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2810bbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g6/6tx7kz_51_92m63qx0q2c2lw0000gn/T/ipykernel_4710/2881964319.py:32: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  score = grouped.apply(\n"
     ]
    }
   ],
   "source": [
    "# produce df with one value per model per item \n",
    "model_item_scores_SOEP = get_LLM_value_per_item(SOEP_data)\n",
    "model_item_scores_SOEP_top_n = get_LLM_value_per_item_top_n(SOEP_data)\n",
    "\n",
    "# Merge them on the grouping keys\n",
    "model_item_scores_SOEP = model_item_scores_SOEP.merge(\n",
    "    model_item_scores_SOEP_top_n,\n",
    "    on=[\"experiment\", \"model\", \"item\"],\n",
    "    how=\"inner\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2d70fee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding task specific categories to save in all data\n",
    "\n",
    "# add item categories\n",
    "item_to_category = {\n",
    "     1: \"SOEP\", 2: \"SOEPdri\", 3: \"SOEPfin\",  4: \"SOEPrec\", 5: \"SOEPocc\",  6: \"SOEPhea\",  7: \"SOEPsoc\"\n",
    "}\n",
    "\n",
    "model_item_scores_SOEP[\"category\"] = model_item_scores_SOEP[\"item\"].map(item_to_category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6ef74f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dfs\n",
    "all_data = pd.concat([all_data, model_item_scores_SOEP], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919cb194",
   "metadata": {},
   "source": [
    "## SSSV SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4ee11e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrame shape: (2776560, 8)\n",
      "Total models: 46\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "SSSV_data = load_dataframes(task_name=\"SSSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dfaf361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise answer option sum to one\n",
    "SSSV_data[\"prob_1\"] = np.exp(SSSV_data[\"1\"])/(np.exp(SSSV_data[\"1\"]) + np.exp(SSSV_data[\"2\"]))\n",
    "SSSV_data[\"prob_2\"] = np.exp(SSSV_data[\"2\"])/(np.exp(SSSV_data[\"1\"]) + np.exp(SSSV_data[\"2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3c2a7fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out probability LLM assigned to real item answer \n",
    "SSSV_data=filter_pred_prob(SSSV_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ef714e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add item categories\n",
    "item_to_category = {\n",
    "     3: \"SStas\", 11: \"SStas\", 16: \"SStas\", 17: \"SStas\", 20: \"SStas\", 21: \"SStas\", 23: \"SStas\", 28: \"SStas\", 38: \"SStas\", 40: \"SStas\",\n",
    "     4: \"SSexp\", 6: \"SSexp\", 9: \"SSexp\", 10: \"SSexp\", 14: \"SSexp\", 18: \"SSexp\", 19: \"SSexp\", 22: \"SSexp\", 26: \"SSexp\", 37: \"SSexp\",\n",
    "     1: \"SSdis\", 12: \"SSdis\", 13: \"SSdis\", 25: \"SSdis\", 29: \"SSdis\", 30: \"SSdis\", 32: \"SSdis\", 33: \"SSdis\", 35: \"SSdis\", 36: \"SSdis\",\n",
    "     2: \"SSbor\", 5: \"SSbor\", 7: \"SSbor\", 8: \"SSbor\", 15: \"SSbor\", 24: \"SSbor\", 27: \"SSbor\", 31: \"SSbor\", 34: \"SSbor\", 39: \"SSbor\"\n",
    "}\n",
    "\n",
    "SSSV_data[\"category\"] = SSSV_data[\"item\"].map(item_to_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bac1cb1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([64000401, 64004701, 64006401, ..., 68051101, 68051301, 68051401])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SSSV_data[\"participant\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cbcfcdab",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sstas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msstas\u001b[49m\u001b[38;5;241m.\u001b[39mtail(\u001b[38;5;241m60\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sstas' is not defined"
     ]
    }
   ],
   "source": [
    "sstas.tail(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c3664f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>item</th>\n",
       "      <th>3</th>\n",
       "      <th>11</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>23</th>\n",
       "      <th>28</th>\n",
       "      <th>38</th>\n",
       "      <th>40</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Apertus-70B-Instruct-2509</th>\n",
       "      <td>0.409748</td>\n",
       "      <td>0.434093</td>\n",
       "      <td>0.372400</td>\n",
       "      <td>0.421103</td>\n",
       "      <td>0.373412</td>\n",
       "      <td>0.376160</td>\n",
       "      <td>0.356125</td>\n",
       "      <td>0.280292</td>\n",
       "      <td>0.343601</td>\n",
       "      <td>0.368510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Apertus-8B-Instruct-2509</th>\n",
       "      <td>0.441008</td>\n",
       "      <td>0.420211</td>\n",
       "      <td>0.431743</td>\n",
       "      <td>0.439153</td>\n",
       "      <td>0.414523</td>\n",
       "      <td>0.437412</td>\n",
       "      <td>0.443728</td>\n",
       "      <td>0.439294</td>\n",
       "      <td>0.438101</td>\n",
       "      <td>0.450987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Falcon-3-10B-Instruct</th>\n",
       "      <td>0.380454</td>\n",
       "      <td>0.255767</td>\n",
       "      <td>0.498293</td>\n",
       "      <td>0.496323</td>\n",
       "      <td>0.274307</td>\n",
       "      <td>0.377427</td>\n",
       "      <td>0.440160</td>\n",
       "      <td>0.448273</td>\n",
       "      <td>0.371459</td>\n",
       "      <td>0.354826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Falcon-3-1B-Instruct</th>\n",
       "      <td>0.427747</td>\n",
       "      <td>0.402559</td>\n",
       "      <td>0.459269</td>\n",
       "      <td>0.499541</td>\n",
       "      <td>0.402028</td>\n",
       "      <td>0.384131</td>\n",
       "      <td>0.423851</td>\n",
       "      <td>0.418666</td>\n",
       "      <td>0.394853</td>\n",
       "      <td>0.384968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Falcon-3-7B-Instruct</th>\n",
       "      <td>0.325699</td>\n",
       "      <td>0.377381</td>\n",
       "      <td>0.395096</td>\n",
       "      <td>0.402361</td>\n",
       "      <td>0.311522</td>\n",
       "      <td>0.300391</td>\n",
       "      <td>0.397327</td>\n",
       "      <td>0.338521</td>\n",
       "      <td>0.377811</td>\n",
       "      <td>0.402715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LFM2-1.2B</th>\n",
       "      <td>0.410329</td>\n",
       "      <td>0.352616</td>\n",
       "      <td>0.364923</td>\n",
       "      <td>0.378916</td>\n",
       "      <td>0.351913</td>\n",
       "      <td>0.323831</td>\n",
       "      <td>0.363025</td>\n",
       "      <td>0.355816</td>\n",
       "      <td>0.334699</td>\n",
       "      <td>0.349026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LFM2-2.6B</th>\n",
       "      <td>0.416827</td>\n",
       "      <td>0.449684</td>\n",
       "      <td>0.389054</td>\n",
       "      <td>0.402680</td>\n",
       "      <td>0.446960</td>\n",
       "      <td>0.420966</td>\n",
       "      <td>0.365345</td>\n",
       "      <td>0.360895</td>\n",
       "      <td>0.456548</td>\n",
       "      <td>0.444869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LFM2-8B-A1B</th>\n",
       "      <td>0.449285</td>\n",
       "      <td>0.366039</td>\n",
       "      <td>0.358431</td>\n",
       "      <td>0.371257</td>\n",
       "      <td>0.381967</td>\n",
       "      <td>0.371773</td>\n",
       "      <td>0.355894</td>\n",
       "      <td>0.355793</td>\n",
       "      <td>0.312250</td>\n",
       "      <td>0.361061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama-3.1-70B-Instruct</th>\n",
       "      <td>0.512106</td>\n",
       "      <td>0.428729</td>\n",
       "      <td>0.502043</td>\n",
       "      <td>0.526888</td>\n",
       "      <td>0.475569</td>\n",
       "      <td>0.435453</td>\n",
       "      <td>0.476562</td>\n",
       "      <td>0.520280</td>\n",
       "      <td>0.484968</td>\n",
       "      <td>0.489794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama-3.1-8B-Instruct</th>\n",
       "      <td>0.488113</td>\n",
       "      <td>0.366247</td>\n",
       "      <td>0.474774</td>\n",
       "      <td>0.508509</td>\n",
       "      <td>0.422338</td>\n",
       "      <td>0.411291</td>\n",
       "      <td>0.457826</td>\n",
       "      <td>0.452989</td>\n",
       "      <td>0.426993</td>\n",
       "      <td>0.404024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama-3.2-1B-Instruct</th>\n",
       "      <td>0.485278</td>\n",
       "      <td>0.419847</td>\n",
       "      <td>0.425979</td>\n",
       "      <td>0.440798</td>\n",
       "      <td>0.439013</td>\n",
       "      <td>0.433007</td>\n",
       "      <td>0.443675</td>\n",
       "      <td>0.435962</td>\n",
       "      <td>0.462743</td>\n",
       "      <td>0.458178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama-3.2-3B-Instruct</th>\n",
       "      <td>0.429573</td>\n",
       "      <td>0.391853</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.463660</td>\n",
       "      <td>0.416442</td>\n",
       "      <td>0.399926</td>\n",
       "      <td>0.473245</td>\n",
       "      <td>0.440516</td>\n",
       "      <td>0.424063</td>\n",
       "      <td>0.472878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama-3.3-70B-Instruct</th>\n",
       "      <td>0.520403</td>\n",
       "      <td>0.471716</td>\n",
       "      <td>0.490606</td>\n",
       "      <td>0.472071</td>\n",
       "      <td>0.486889</td>\n",
       "      <td>0.472217</td>\n",
       "      <td>0.444222</td>\n",
       "      <td>0.558213</td>\n",
       "      <td>0.541202</td>\n",
       "      <td>0.499973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ministral-8B-Instruct-2410</th>\n",
       "      <td>0.431897</td>\n",
       "      <td>0.479063</td>\n",
       "      <td>0.386883</td>\n",
       "      <td>0.443339</td>\n",
       "      <td>0.468241</td>\n",
       "      <td>0.440450</td>\n",
       "      <td>0.411165</td>\n",
       "      <td>0.452267</td>\n",
       "      <td>0.468069</td>\n",
       "      <td>0.473764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mistral-7B-Instruct-v0.3</th>\n",
       "      <td>0.539301</td>\n",
       "      <td>0.466579</td>\n",
       "      <td>0.600323</td>\n",
       "      <td>0.688263</td>\n",
       "      <td>0.456819</td>\n",
       "      <td>0.419559</td>\n",
       "      <td>0.625539</td>\n",
       "      <td>0.517732</td>\n",
       "      <td>0.409035</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mistral-Small-24B-Instruct-2501</th>\n",
       "      <td>0.415497</td>\n",
       "      <td>0.371477</td>\n",
       "      <td>0.425103</td>\n",
       "      <td>0.449838</td>\n",
       "      <td>0.404400</td>\n",
       "      <td>0.415109</td>\n",
       "      <td>0.419657</td>\n",
       "      <td>0.414555</td>\n",
       "      <td>0.412940</td>\n",
       "      <td>0.406798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OLMo-2-7B-Instruct</th>\n",
       "      <td>0.482760</td>\n",
       "      <td>0.445740</td>\n",
       "      <td>0.462668</td>\n",
       "      <td>0.448209</td>\n",
       "      <td>0.439711</td>\n",
       "      <td>0.443087</td>\n",
       "      <td>0.447875</td>\n",
       "      <td>0.455797</td>\n",
       "      <td>0.428115</td>\n",
       "      <td>0.423762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phi-3-medium-128k-instruct</th>\n",
       "      <td>0.452965</td>\n",
       "      <td>0.414258</td>\n",
       "      <td>0.436681</td>\n",
       "      <td>0.456650</td>\n",
       "      <td>0.455205</td>\n",
       "      <td>0.442466</td>\n",
       "      <td>0.456913</td>\n",
       "      <td>0.421660</td>\n",
       "      <td>0.491228</td>\n",
       "      <td>0.475951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phi-3-mini-128k-instruct</th>\n",
       "      <td>0.408941</td>\n",
       "      <td>0.431406</td>\n",
       "      <td>0.487099</td>\n",
       "      <td>0.477943</td>\n",
       "      <td>0.464405</td>\n",
       "      <td>0.437312</td>\n",
       "      <td>0.482150</td>\n",
       "      <td>0.466430</td>\n",
       "      <td>0.455772</td>\n",
       "      <td>0.448521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phi-3.5-mini-instruct</th>\n",
       "      <td>0.459684</td>\n",
       "      <td>0.403183</td>\n",
       "      <td>0.402719</td>\n",
       "      <td>0.451045</td>\n",
       "      <td>0.444976</td>\n",
       "      <td>0.465988</td>\n",
       "      <td>0.445796</td>\n",
       "      <td>0.428864</td>\n",
       "      <td>0.443084</td>\n",
       "      <td>0.423290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen2.5-1.5B-Instruct</th>\n",
       "      <td>0.381396</td>\n",
       "      <td>0.303965</td>\n",
       "      <td>0.318248</td>\n",
       "      <td>0.378813</td>\n",
       "      <td>0.327055</td>\n",
       "      <td>0.356603</td>\n",
       "      <td>0.338234</td>\n",
       "      <td>0.372801</td>\n",
       "      <td>0.345909</td>\n",
       "      <td>0.410432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen2.5-14B-Instruct</th>\n",
       "      <td>0.350055</td>\n",
       "      <td>0.419478</td>\n",
       "      <td>0.462066</td>\n",
       "      <td>0.424131</td>\n",
       "      <td>0.390165</td>\n",
       "      <td>0.412630</td>\n",
       "      <td>0.388502</td>\n",
       "      <td>0.391779</td>\n",
       "      <td>0.396432</td>\n",
       "      <td>0.402634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen2.5-32B-Instruct</th>\n",
       "      <td>0.374707</td>\n",
       "      <td>0.401352</td>\n",
       "      <td>0.424979</td>\n",
       "      <td>0.429796</td>\n",
       "      <td>0.415029</td>\n",
       "      <td>0.399720</td>\n",
       "      <td>0.439825</td>\n",
       "      <td>0.409114</td>\n",
       "      <td>0.423020</td>\n",
       "      <td>0.448941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen2.5-3B-Instruct</th>\n",
       "      <td>0.456643</td>\n",
       "      <td>0.415279</td>\n",
       "      <td>0.414622</td>\n",
       "      <td>0.420720</td>\n",
       "      <td>0.412547</td>\n",
       "      <td>0.417888</td>\n",
       "      <td>0.427275</td>\n",
       "      <td>0.435416</td>\n",
       "      <td>0.431133</td>\n",
       "      <td>0.445529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen2.5-7B-Instruct</th>\n",
       "      <td>0.362995</td>\n",
       "      <td>0.330456</td>\n",
       "      <td>0.315341</td>\n",
       "      <td>0.433182</td>\n",
       "      <td>0.397652</td>\n",
       "      <td>0.374097</td>\n",
       "      <td>0.374157</td>\n",
       "      <td>0.299196</td>\n",
       "      <td>0.284518</td>\n",
       "      <td>0.318786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen3-1.7B</th>\n",
       "      <td>0.466583</td>\n",
       "      <td>0.470393</td>\n",
       "      <td>0.486559</td>\n",
       "      <td>0.492446</td>\n",
       "      <td>0.459682</td>\n",
       "      <td>0.454717</td>\n",
       "      <td>0.478178</td>\n",
       "      <td>0.488011</td>\n",
       "      <td>0.458799</td>\n",
       "      <td>0.465367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen3-14B</th>\n",
       "      <td>0.476888</td>\n",
       "      <td>0.411664</td>\n",
       "      <td>0.404328</td>\n",
       "      <td>0.452437</td>\n",
       "      <td>0.418548</td>\n",
       "      <td>0.458023</td>\n",
       "      <td>0.445818</td>\n",
       "      <td>0.424433</td>\n",
       "      <td>0.428253</td>\n",
       "      <td>0.434076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen3-30B-A3B-Instruct-2507</th>\n",
       "      <td>0.447479</td>\n",
       "      <td>0.464535</td>\n",
       "      <td>0.487427</td>\n",
       "      <td>0.492193</td>\n",
       "      <td>0.459620</td>\n",
       "      <td>0.475425</td>\n",
       "      <td>0.481843</td>\n",
       "      <td>0.487747</td>\n",
       "      <td>0.480975</td>\n",
       "      <td>0.487997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen3-32B</th>\n",
       "      <td>0.305150</td>\n",
       "      <td>0.231074</td>\n",
       "      <td>0.356641</td>\n",
       "      <td>0.343815</td>\n",
       "      <td>0.227579</td>\n",
       "      <td>0.225781</td>\n",
       "      <td>0.326072</td>\n",
       "      <td>0.331518</td>\n",
       "      <td>0.277952</td>\n",
       "      <td>0.309831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen3-4B</th>\n",
       "      <td>0.421332</td>\n",
       "      <td>0.451412</td>\n",
       "      <td>0.427805</td>\n",
       "      <td>0.437986</td>\n",
       "      <td>0.467558</td>\n",
       "      <td>0.483291</td>\n",
       "      <td>0.425358</td>\n",
       "      <td>0.446691</td>\n",
       "      <td>0.441827</td>\n",
       "      <td>0.454676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen3-8B</th>\n",
       "      <td>0.676732</td>\n",
       "      <td>0.534308</td>\n",
       "      <td>0.430099</td>\n",
       "      <td>0.459086</td>\n",
       "      <td>0.450767</td>\n",
       "      <td>0.482028</td>\n",
       "      <td>0.457772</td>\n",
       "      <td>0.506258</td>\n",
       "      <td>0.508786</td>\n",
       "      <td>0.558669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SmolLM3-3B</th>\n",
       "      <td>0.464470</td>\n",
       "      <td>0.483549</td>\n",
       "      <td>0.468655</td>\n",
       "      <td>0.483409</td>\n",
       "      <td>0.483859</td>\n",
       "      <td>0.488283</td>\n",
       "      <td>0.460427</td>\n",
       "      <td>0.460929</td>\n",
       "      <td>0.498760</td>\n",
       "      <td>0.487106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TildeOpen-30b</th>\n",
       "      <td>0.446475</td>\n",
       "      <td>0.429030</td>\n",
       "      <td>0.453002</td>\n",
       "      <td>0.475452</td>\n",
       "      <td>0.443382</td>\n",
       "      <td>0.437436</td>\n",
       "      <td>0.438980</td>\n",
       "      <td>0.455872</td>\n",
       "      <td>0.414208</td>\n",
       "      <td>0.434070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bloomz-3b</th>\n",
       "      <td>0.442554</td>\n",
       "      <td>0.422752</td>\n",
       "      <td>0.631001</td>\n",
       "      <td>0.700721</td>\n",
       "      <td>0.488863</td>\n",
       "      <td>0.470299</td>\n",
       "      <td>0.661494</td>\n",
       "      <td>0.530559</td>\n",
       "      <td>0.356739</td>\n",
       "      <td>0.496317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bloomz-7b1</th>\n",
       "      <td>0.587846</td>\n",
       "      <td>0.374508</td>\n",
       "      <td>0.636948</td>\n",
       "      <td>0.707882</td>\n",
       "      <td>0.427629</td>\n",
       "      <td>0.435281</td>\n",
       "      <td>0.675615</td>\n",
       "      <td>0.541774</td>\n",
       "      <td>0.391432</td>\n",
       "      <td>0.503637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-2-27b-it</th>\n",
       "      <td>0.424637</td>\n",
       "      <td>0.342485</td>\n",
       "      <td>0.550467</td>\n",
       "      <td>0.637396</td>\n",
       "      <td>0.385833</td>\n",
       "      <td>0.369483</td>\n",
       "      <td>0.526029</td>\n",
       "      <td>0.438170</td>\n",
       "      <td>0.354417</td>\n",
       "      <td>0.391453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-2-2b-it</th>\n",
       "      <td>0.434468</td>\n",
       "      <td>0.456768</td>\n",
       "      <td>0.436848</td>\n",
       "      <td>0.462156</td>\n",
       "      <td>0.443740</td>\n",
       "      <td>0.456451</td>\n",
       "      <td>0.457094</td>\n",
       "      <td>0.453800</td>\n",
       "      <td>0.428414</td>\n",
       "      <td>0.450988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-2-9b-it</th>\n",
       "      <td>0.451085</td>\n",
       "      <td>0.463024</td>\n",
       "      <td>0.513022</td>\n",
       "      <td>0.544959</td>\n",
       "      <td>0.510315</td>\n",
       "      <td>0.494214</td>\n",
       "      <td>0.552858</td>\n",
       "      <td>0.535537</td>\n",
       "      <td>0.496377</td>\n",
       "      <td>0.531382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-3-12b-it</th>\n",
       "      <td>0.628038</td>\n",
       "      <td>0.503516</td>\n",
       "      <td>0.544539</td>\n",
       "      <td>0.558932</td>\n",
       "      <td>0.502370</td>\n",
       "      <td>0.480964</td>\n",
       "      <td>0.556304</td>\n",
       "      <td>0.517393</td>\n",
       "      <td>0.489771</td>\n",
       "      <td>0.494428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-3-1b-it</th>\n",
       "      <td>0.408905</td>\n",
       "      <td>0.461392</td>\n",
       "      <td>0.388642</td>\n",
       "      <td>0.425002</td>\n",
       "      <td>0.493275</td>\n",
       "      <td>0.531040</td>\n",
       "      <td>0.438623</td>\n",
       "      <td>0.476387</td>\n",
       "      <td>0.557458</td>\n",
       "      <td>0.531962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-3-27b-it</th>\n",
       "      <td>0.563701</td>\n",
       "      <td>0.455123</td>\n",
       "      <td>0.475543</td>\n",
       "      <td>0.493310</td>\n",
       "      <td>0.466739</td>\n",
       "      <td>0.475714</td>\n",
       "      <td>0.491341</td>\n",
       "      <td>0.474715</td>\n",
       "      <td>0.487710</td>\n",
       "      <td>0.486943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-3-4b-it</th>\n",
       "      <td>0.504422</td>\n",
       "      <td>0.466650</td>\n",
       "      <td>0.537415</td>\n",
       "      <td>0.553018</td>\n",
       "      <td>0.582675</td>\n",
       "      <td>0.556489</td>\n",
       "      <td>0.504436</td>\n",
       "      <td>0.512876</td>\n",
       "      <td>0.544903</td>\n",
       "      <td>0.533770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-oss-20b</th>\n",
       "      <td>0.379871</td>\n",
       "      <td>0.408552</td>\n",
       "      <td>0.327449</td>\n",
       "      <td>0.329298</td>\n",
       "      <td>0.386842</td>\n",
       "      <td>0.416146</td>\n",
       "      <td>0.311231</td>\n",
       "      <td>0.333950</td>\n",
       "      <td>0.435168</td>\n",
       "      <td>0.363867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>granite-3.3-2b-instruct</th>\n",
       "      <td>0.417161</td>\n",
       "      <td>0.453637</td>\n",
       "      <td>0.491367</td>\n",
       "      <td>0.499546</td>\n",
       "      <td>0.473485</td>\n",
       "      <td>0.473519</td>\n",
       "      <td>0.536598</td>\n",
       "      <td>0.475817</td>\n",
       "      <td>0.459827</td>\n",
       "      <td>0.507009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>granite-3.3-8b-instruct</th>\n",
       "      <td>0.707570</td>\n",
       "      <td>0.709181</td>\n",
       "      <td>0.632989</td>\n",
       "      <td>0.654912</td>\n",
       "      <td>0.829483</td>\n",
       "      <td>0.664582</td>\n",
       "      <td>0.644452</td>\n",
       "      <td>0.548313</td>\n",
       "      <td>0.551002</td>\n",
       "      <td>0.548313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zephyr-7b-beta</th>\n",
       "      <td>0.488284</td>\n",
       "      <td>0.471590</td>\n",
       "      <td>0.541514</td>\n",
       "      <td>0.560005</td>\n",
       "      <td>0.468455</td>\n",
       "      <td>0.462135</td>\n",
       "      <td>0.539328</td>\n",
       "      <td>0.504008</td>\n",
       "      <td>0.460346</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "item                                   3         11        16        17  \\\n",
       "model                                                                     \n",
       "Apertus-70B-Instruct-2509        0.409748  0.434093  0.372400  0.421103   \n",
       "Apertus-8B-Instruct-2509         0.441008  0.420211  0.431743  0.439153   \n",
       "Falcon-3-10B-Instruct            0.380454  0.255767  0.498293  0.496323   \n",
       "Falcon-3-1B-Instruct             0.427747  0.402559  0.459269  0.499541   \n",
       "Falcon-3-7B-Instruct             0.325699  0.377381  0.395096  0.402361   \n",
       "LFM2-1.2B                        0.410329  0.352616  0.364923  0.378916   \n",
       "LFM2-2.6B                        0.416827  0.449684  0.389054  0.402680   \n",
       "LFM2-8B-A1B                      0.449285  0.366039  0.358431  0.371257   \n",
       "Llama-3.1-70B-Instruct           0.512106  0.428729  0.502043  0.526888   \n",
       "Llama-3.1-8B-Instruct            0.488113  0.366247  0.474774  0.508509   \n",
       "Llama-3.2-1B-Instruct            0.485278  0.419847  0.425979  0.440798   \n",
       "Llama-3.2-3B-Instruct            0.429573  0.391853  0.434783  0.463660   \n",
       "Llama-3.3-70B-Instruct           0.520403  0.471716  0.490606  0.472071   \n",
       "Ministral-8B-Instruct-2410       0.431897  0.479063  0.386883  0.443339   \n",
       "Mistral-7B-Instruct-v0.3         0.539301  0.466579  0.600323  0.688263   \n",
       "Mistral-Small-24B-Instruct-2501  0.415497  0.371477  0.425103  0.449838   \n",
       "OLMo-2-7B-Instruct               0.482760  0.445740  0.462668  0.448209   \n",
       "Phi-3-medium-128k-instruct       0.452965  0.414258  0.436681  0.456650   \n",
       "Phi-3-mini-128k-instruct         0.408941  0.431406  0.487099  0.477943   \n",
       "Phi-3.5-mini-instruct            0.459684  0.403183  0.402719  0.451045   \n",
       "Qwen2.5-1.5B-Instruct            0.381396  0.303965  0.318248  0.378813   \n",
       "Qwen2.5-14B-Instruct             0.350055  0.419478  0.462066  0.424131   \n",
       "Qwen2.5-32B-Instruct             0.374707  0.401352  0.424979  0.429796   \n",
       "Qwen2.5-3B-Instruct              0.456643  0.415279  0.414622  0.420720   \n",
       "Qwen2.5-7B-Instruct              0.362995  0.330456  0.315341  0.433182   \n",
       "Qwen3-1.7B                       0.466583  0.470393  0.486559  0.492446   \n",
       "Qwen3-14B                        0.476888  0.411664  0.404328  0.452437   \n",
       "Qwen3-30B-A3B-Instruct-2507      0.447479  0.464535  0.487427  0.492193   \n",
       "Qwen3-32B                        0.305150  0.231074  0.356641  0.343815   \n",
       "Qwen3-4B                         0.421332  0.451412  0.427805  0.437986   \n",
       "Qwen3-8B                         0.676732  0.534308  0.430099  0.459086   \n",
       "SmolLM3-3B                       0.464470  0.483549  0.468655  0.483409   \n",
       "TildeOpen-30b                    0.446475  0.429030  0.453002  0.475452   \n",
       "bloomz-3b                        0.442554  0.422752  0.631001  0.700721   \n",
       "bloomz-7b1                       0.587846  0.374508  0.636948  0.707882   \n",
       "gemma-2-27b-it                   0.424637  0.342485  0.550467  0.637396   \n",
       "gemma-2-2b-it                    0.434468  0.456768  0.436848  0.462156   \n",
       "gemma-2-9b-it                    0.451085  0.463024  0.513022  0.544959   \n",
       "gemma-3-12b-it                   0.628038  0.503516  0.544539  0.558932   \n",
       "gemma-3-1b-it                    0.408905  0.461392  0.388642  0.425002   \n",
       "gemma-3-27b-it                   0.563701  0.455123  0.475543  0.493310   \n",
       "gemma-3-4b-it                    0.504422  0.466650  0.537415  0.553018   \n",
       "gpt-oss-20b                      0.379871  0.408552  0.327449  0.329298   \n",
       "granite-3.3-2b-instruct          0.417161  0.453637  0.491367  0.499546   \n",
       "granite-3.3-8b-instruct          0.707570  0.709181  0.632989  0.654912   \n",
       "zephyr-7b-beta                   0.488284  0.471590  0.541514  0.560005   \n",
       "\n",
       "item                                   20        21        23        28  \\\n",
       "model                                                                     \n",
       "Apertus-70B-Instruct-2509        0.373412  0.376160  0.356125  0.280292   \n",
       "Apertus-8B-Instruct-2509         0.414523  0.437412  0.443728  0.439294   \n",
       "Falcon-3-10B-Instruct            0.274307  0.377427  0.440160  0.448273   \n",
       "Falcon-3-1B-Instruct             0.402028  0.384131  0.423851  0.418666   \n",
       "Falcon-3-7B-Instruct             0.311522  0.300391  0.397327  0.338521   \n",
       "LFM2-1.2B                        0.351913  0.323831  0.363025  0.355816   \n",
       "LFM2-2.6B                        0.446960  0.420966  0.365345  0.360895   \n",
       "LFM2-8B-A1B                      0.381967  0.371773  0.355894  0.355793   \n",
       "Llama-3.1-70B-Instruct           0.475569  0.435453  0.476562  0.520280   \n",
       "Llama-3.1-8B-Instruct            0.422338  0.411291  0.457826  0.452989   \n",
       "Llama-3.2-1B-Instruct            0.439013  0.433007  0.443675  0.435962   \n",
       "Llama-3.2-3B-Instruct            0.416442  0.399926  0.473245  0.440516   \n",
       "Llama-3.3-70B-Instruct           0.486889  0.472217  0.444222  0.558213   \n",
       "Ministral-8B-Instruct-2410       0.468241  0.440450  0.411165  0.452267   \n",
       "Mistral-7B-Instruct-v0.3         0.456819  0.419559  0.625539  0.517732   \n",
       "Mistral-Small-24B-Instruct-2501  0.404400  0.415109  0.419657  0.414555   \n",
       "OLMo-2-7B-Instruct               0.439711  0.443087  0.447875  0.455797   \n",
       "Phi-3-medium-128k-instruct       0.455205  0.442466  0.456913  0.421660   \n",
       "Phi-3-mini-128k-instruct         0.464405  0.437312  0.482150  0.466430   \n",
       "Phi-3.5-mini-instruct            0.444976  0.465988  0.445796  0.428864   \n",
       "Qwen2.5-1.5B-Instruct            0.327055  0.356603  0.338234  0.372801   \n",
       "Qwen2.5-14B-Instruct             0.390165  0.412630  0.388502  0.391779   \n",
       "Qwen2.5-32B-Instruct             0.415029  0.399720  0.439825  0.409114   \n",
       "Qwen2.5-3B-Instruct              0.412547  0.417888  0.427275  0.435416   \n",
       "Qwen2.5-7B-Instruct              0.397652  0.374097  0.374157  0.299196   \n",
       "Qwen3-1.7B                       0.459682  0.454717  0.478178  0.488011   \n",
       "Qwen3-14B                        0.418548  0.458023  0.445818  0.424433   \n",
       "Qwen3-30B-A3B-Instruct-2507      0.459620  0.475425  0.481843  0.487747   \n",
       "Qwen3-32B                        0.227579  0.225781  0.326072  0.331518   \n",
       "Qwen3-4B                         0.467558  0.483291  0.425358  0.446691   \n",
       "Qwen3-8B                         0.450767  0.482028  0.457772  0.506258   \n",
       "SmolLM3-3B                       0.483859  0.488283  0.460427  0.460929   \n",
       "TildeOpen-30b                    0.443382  0.437436  0.438980  0.455872   \n",
       "bloomz-3b                        0.488863  0.470299  0.661494  0.530559   \n",
       "bloomz-7b1                       0.427629  0.435281  0.675615  0.541774   \n",
       "gemma-2-27b-it                   0.385833  0.369483  0.526029  0.438170   \n",
       "gemma-2-2b-it                    0.443740  0.456451  0.457094  0.453800   \n",
       "gemma-2-9b-it                    0.510315  0.494214  0.552858  0.535537   \n",
       "gemma-3-12b-it                   0.502370  0.480964  0.556304  0.517393   \n",
       "gemma-3-1b-it                    0.493275  0.531040  0.438623  0.476387   \n",
       "gemma-3-27b-it                   0.466739  0.475714  0.491341  0.474715   \n",
       "gemma-3-4b-it                    0.582675  0.556489  0.504436  0.512876   \n",
       "gpt-oss-20b                      0.386842  0.416146  0.311231  0.333950   \n",
       "granite-3.3-2b-instruct          0.473485  0.473519  0.536598  0.475817   \n",
       "granite-3.3-8b-instruct          0.829483  0.664582  0.644452  0.548313   \n",
       "zephyr-7b-beta                   0.468455  0.462135  0.539328  0.504008   \n",
       "\n",
       "item                                   38        40  \n",
       "model                                                \n",
       "Apertus-70B-Instruct-2509        0.343601  0.368510  \n",
       "Apertus-8B-Instruct-2509         0.438101  0.450987  \n",
       "Falcon-3-10B-Instruct            0.371459  0.354826  \n",
       "Falcon-3-1B-Instruct             0.394853  0.384968  \n",
       "Falcon-3-7B-Instruct             0.377811  0.402715  \n",
       "LFM2-1.2B                        0.334699  0.349026  \n",
       "LFM2-2.6B                        0.456548  0.444869  \n",
       "LFM2-8B-A1B                      0.312250  0.361061  \n",
       "Llama-3.1-70B-Instruct           0.484968  0.489794  \n",
       "Llama-3.1-8B-Instruct            0.426993  0.404024  \n",
       "Llama-3.2-1B-Instruct            0.462743  0.458178  \n",
       "Llama-3.2-3B-Instruct            0.424063  0.472878  \n",
       "Llama-3.3-70B-Instruct           0.541202  0.499973  \n",
       "Ministral-8B-Instruct-2410       0.468069  0.473764  \n",
       "Mistral-7B-Instruct-v0.3         0.409035  0.500000  \n",
       "Mistral-Small-24B-Instruct-2501  0.412940  0.406798  \n",
       "OLMo-2-7B-Instruct               0.428115  0.423762  \n",
       "Phi-3-medium-128k-instruct       0.491228  0.475951  \n",
       "Phi-3-mini-128k-instruct         0.455772  0.448521  \n",
       "Phi-3.5-mini-instruct            0.443084  0.423290  \n",
       "Qwen2.5-1.5B-Instruct            0.345909  0.410432  \n",
       "Qwen2.5-14B-Instruct             0.396432  0.402634  \n",
       "Qwen2.5-32B-Instruct             0.423020  0.448941  \n",
       "Qwen2.5-3B-Instruct              0.431133  0.445529  \n",
       "Qwen2.5-7B-Instruct              0.284518  0.318786  \n",
       "Qwen3-1.7B                       0.458799  0.465367  \n",
       "Qwen3-14B                        0.428253  0.434076  \n",
       "Qwen3-30B-A3B-Instruct-2507      0.480975  0.487997  \n",
       "Qwen3-32B                        0.277952  0.309831  \n",
       "Qwen3-4B                         0.441827  0.454676  \n",
       "Qwen3-8B                         0.508786  0.558669  \n",
       "SmolLM3-3B                       0.498760  0.487106  \n",
       "TildeOpen-30b                    0.414208  0.434070  \n",
       "bloomz-3b                        0.356739  0.496317  \n",
       "bloomz-7b1                       0.391432  0.503637  \n",
       "gemma-2-27b-it                   0.354417  0.391453  \n",
       "gemma-2-2b-it                    0.428414  0.450988  \n",
       "gemma-2-9b-it                    0.496377  0.531382  \n",
       "gemma-3-12b-it                   0.489771  0.494428  \n",
       "gemma-3-1b-it                    0.557458  0.531962  \n",
       "gemma-3-27b-it                   0.487710  0.486943  \n",
       "gemma-3-4b-it                    0.544903  0.533770  \n",
       "gpt-oss-20b                      0.435168  0.363867  \n",
       "granite-3.3-2b-instruct          0.459827  0.507009  \n",
       "granite-3.3-8b-instruct          0.551002  0.548313  \n",
       "zephyr-7b-beta                   0.460346  0.500000  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sstas= SSSV_data[SSSV_data[\"category\"] == \"SStas\"]\n",
    "df_wide = sstas.pivot_table(index='model', columns='item', values='prob_pred')\n",
    "df_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e14c5a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>item</th>\n",
       "      <th>3</th>\n",
       "      <th>11</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>23</th>\n",
       "      <th>28</th>\n",
       "      <th>38</th>\n",
       "      <th>40</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.698952</td>\n",
       "      <td>0.586719</td>\n",
       "      <td>0.570461</td>\n",
       "      <td>0.691551</td>\n",
       "      <td>0.669476</td>\n",
       "      <td>0.628829</td>\n",
       "      <td>0.692664</td>\n",
       "      <td>0.565050</td>\n",
       "      <td>0.672881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.698952</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.435148</td>\n",
       "      <td>0.359785</td>\n",
       "      <td>0.894286</td>\n",
       "      <td>0.849056</td>\n",
       "      <td>0.474746</td>\n",
       "      <td>0.566564</td>\n",
       "      <td>0.759233</td>\n",
       "      <td>0.757589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.586719</td>\n",
       "      <td>0.435148</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.940401</td>\n",
       "      <td>0.557489</td>\n",
       "      <td>0.515794</td>\n",
       "      <td>0.935632</td>\n",
       "      <td>0.813937</td>\n",
       "      <td>0.347336</td>\n",
       "      <td>0.598080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.570461</td>\n",
       "      <td>0.359785</td>\n",
       "      <td>0.940401</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.517764</td>\n",
       "      <td>0.458775</td>\n",
       "      <td>0.947146</td>\n",
       "      <td>0.730886</td>\n",
       "      <td>0.215247</td>\n",
       "      <td>0.538264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.691551</td>\n",
       "      <td>0.894286</td>\n",
       "      <td>0.557489</td>\n",
       "      <td>0.517764</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.921822</td>\n",
       "      <td>0.606859</td>\n",
       "      <td>0.650395</td>\n",
       "      <td>0.730959</td>\n",
       "      <td>0.745173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.669476</td>\n",
       "      <td>0.849056</td>\n",
       "      <td>0.515794</td>\n",
       "      <td>0.458775</td>\n",
       "      <td>0.921822</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.572730</td>\n",
       "      <td>0.706517</td>\n",
       "      <td>0.824043</td>\n",
       "      <td>0.802409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.628829</td>\n",
       "      <td>0.474746</td>\n",
       "      <td>0.935632</td>\n",
       "      <td>0.947146</td>\n",
       "      <td>0.606859</td>\n",
       "      <td>0.572730</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.817175</td>\n",
       "      <td>0.359257</td>\n",
       "      <td>0.688779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.692664</td>\n",
       "      <td>0.566564</td>\n",
       "      <td>0.813937</td>\n",
       "      <td>0.730886</td>\n",
       "      <td>0.650395</td>\n",
       "      <td>0.706517</td>\n",
       "      <td>0.817175</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.678537</td>\n",
       "      <td>0.838658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.565050</td>\n",
       "      <td>0.759233</td>\n",
       "      <td>0.347336</td>\n",
       "      <td>0.215247</td>\n",
       "      <td>0.730959</td>\n",
       "      <td>0.824043</td>\n",
       "      <td>0.359257</td>\n",
       "      <td>0.678537</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.831230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.672881</td>\n",
       "      <td>0.757589</td>\n",
       "      <td>0.598080</td>\n",
       "      <td>0.538264</td>\n",
       "      <td>0.745173</td>\n",
       "      <td>0.802409</td>\n",
       "      <td>0.688779</td>\n",
       "      <td>0.838658</td>\n",
       "      <td>0.831230</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "item        3         11        16        17        20        21        23  \\\n",
       "item                                                                         \n",
       "3     1.000000  0.698952  0.586719  0.570461  0.691551  0.669476  0.628829   \n",
       "11    0.698952  1.000000  0.435148  0.359785  0.894286  0.849056  0.474746   \n",
       "16    0.586719  0.435148  1.000000  0.940401  0.557489  0.515794  0.935632   \n",
       "17    0.570461  0.359785  0.940401  1.000000  0.517764  0.458775  0.947146   \n",
       "20    0.691551  0.894286  0.557489  0.517764  1.000000  0.921822  0.606859   \n",
       "21    0.669476  0.849056  0.515794  0.458775  0.921822  1.000000  0.572730   \n",
       "23    0.628829  0.474746  0.935632  0.947146  0.606859  0.572730  1.000000   \n",
       "28    0.692664  0.566564  0.813937  0.730886  0.650395  0.706517  0.817175   \n",
       "38    0.565050  0.759233  0.347336  0.215247  0.730959  0.824043  0.359257   \n",
       "40    0.672881  0.757589  0.598080  0.538264  0.745173  0.802409  0.688779   \n",
       "\n",
       "item        28        38        40  \n",
       "item                                \n",
       "3     0.692664  0.565050  0.672881  \n",
       "11    0.566564  0.759233  0.757589  \n",
       "16    0.813937  0.347336  0.598080  \n",
       "17    0.730886  0.215247  0.538264  \n",
       "20    0.650395  0.730959  0.745173  \n",
       "21    0.706517  0.824043  0.802409  \n",
       "23    0.817175  0.359257  0.688779  \n",
       "28    1.000000  0.678537  0.838658  \n",
       "38    0.678537  1.000000  0.831230  \n",
       "40    0.838658  0.831230  1.000000  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations = df_wide.corr()\n",
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e838ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>item</th>\n",
       "      <th>3</th>\n",
       "      <th>11</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>23</th>\n",
       "      <th>28</th>\n",
       "      <th>38</th>\n",
       "      <th>40</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.610848</td>\n",
       "      <td>0.618287</td>\n",
       "      <td>0.689152</td>\n",
       "      <td>-0.292428</td>\n",
       "      <td>-0.199433</td>\n",
       "      <td>0.598919</td>\n",
       "      <td>0.513943</td>\n",
       "      <td>-0.414697</td>\n",
       "      <td>-0.263115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.610848</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.816615</td>\n",
       "      <td>0.831037</td>\n",
       "      <td>-0.689810</td>\n",
       "      <td>-0.699595</td>\n",
       "      <td>0.841074</td>\n",
       "      <td>0.879496</td>\n",
       "      <td>-0.676049</td>\n",
       "      <td>-0.648194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.618287</td>\n",
       "      <td>0.816615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.911395</td>\n",
       "      <td>-0.520246</td>\n",
       "      <td>-0.631113</td>\n",
       "      <td>0.925372</td>\n",
       "      <td>0.873825</td>\n",
       "      <td>-0.649232</td>\n",
       "      <td>-0.548757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.689152</td>\n",
       "      <td>0.831037</td>\n",
       "      <td>0.911395</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.572903</td>\n",
       "      <td>-0.604025</td>\n",
       "      <td>0.880752</td>\n",
       "      <td>0.879850</td>\n",
       "      <td>-0.656717</td>\n",
       "      <td>-0.591097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.292428</td>\n",
       "      <td>-0.689810</td>\n",
       "      <td>-0.520246</td>\n",
       "      <td>-0.572903</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.792821</td>\n",
       "      <td>-0.455551</td>\n",
       "      <td>-0.522559</td>\n",
       "      <td>0.606443</td>\n",
       "      <td>0.641329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.199433</td>\n",
       "      <td>-0.699595</td>\n",
       "      <td>-0.631113</td>\n",
       "      <td>-0.604025</td>\n",
       "      <td>0.792821</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.597388</td>\n",
       "      <td>-0.637200</td>\n",
       "      <td>0.709501</td>\n",
       "      <td>0.682587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.598919</td>\n",
       "      <td>0.841074</td>\n",
       "      <td>0.925372</td>\n",
       "      <td>0.880752</td>\n",
       "      <td>-0.455551</td>\n",
       "      <td>-0.597388</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.932420</td>\n",
       "      <td>-0.666567</td>\n",
       "      <td>-0.576128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.513943</td>\n",
       "      <td>0.879496</td>\n",
       "      <td>0.873825</td>\n",
       "      <td>0.879850</td>\n",
       "      <td>-0.522559</td>\n",
       "      <td>-0.637200</td>\n",
       "      <td>0.932420</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.691908</td>\n",
       "      <td>-0.641806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.414697</td>\n",
       "      <td>-0.676049</td>\n",
       "      <td>-0.649232</td>\n",
       "      <td>-0.656717</td>\n",
       "      <td>0.606443</td>\n",
       "      <td>0.709501</td>\n",
       "      <td>-0.666567</td>\n",
       "      <td>-0.691908</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.900896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.263115</td>\n",
       "      <td>-0.648194</td>\n",
       "      <td>-0.548757</td>\n",
       "      <td>-0.591097</td>\n",
       "      <td>0.641329</td>\n",
       "      <td>0.682587</td>\n",
       "      <td>-0.576128</td>\n",
       "      <td>-0.641806</td>\n",
       "      <td>0.900896</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "item        3         11        16        17        20        21        23  \\\n",
       "item                                                                         \n",
       "3     1.000000  0.610848  0.618287  0.689152 -0.292428 -0.199433  0.598919   \n",
       "11    0.610848  1.000000  0.816615  0.831037 -0.689810 -0.699595  0.841074   \n",
       "16    0.618287  0.816615  1.000000  0.911395 -0.520246 -0.631113  0.925372   \n",
       "17    0.689152  0.831037  0.911395  1.000000 -0.572903 -0.604025  0.880752   \n",
       "20   -0.292428 -0.689810 -0.520246 -0.572903  1.000000  0.792821 -0.455551   \n",
       "21   -0.199433 -0.699595 -0.631113 -0.604025  0.792821  1.000000 -0.597388   \n",
       "23    0.598919  0.841074  0.925372  0.880752 -0.455551 -0.597388  1.000000   \n",
       "28    0.513943  0.879496  0.873825  0.879850 -0.522559 -0.637200  0.932420   \n",
       "38   -0.414697 -0.676049 -0.649232 -0.656717  0.606443  0.709501 -0.666567   \n",
       "40   -0.263115 -0.648194 -0.548757 -0.591097  0.641329  0.682587 -0.576128   \n",
       "\n",
       "item        28        38        40  \n",
       "item                                \n",
       "3     0.513943 -0.414697 -0.263115  \n",
       "11    0.879496 -0.676049 -0.648194  \n",
       "16    0.873825 -0.649232 -0.548757  \n",
       "17    0.879850 -0.656717 -0.591097  \n",
       "20   -0.522559  0.606443  0.641329  \n",
       "21   -0.637200  0.709501  0.682587  \n",
       "23    0.932420 -0.666567 -0.576128  \n",
       "28    1.000000 -0.691908 -0.641806  \n",
       "38   -0.691908  1.000000  0.900896  \n",
       "40   -0.641806  0.900896  1.000000  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations = df_wide.corr()\n",
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03577688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_number</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>model</th>\n",
       "      <th>item</th>\n",
       "      <th>participant</th>\n",
       "      <th>flipped</th>\n",
       "      <th>experiment</th>\n",
       "      <th>prob_1</th>\n",
       "      <th>prob_2</th>\n",
       "      <th>prob_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-31.3750</td>\n",
       "      <td>-30.500</td>\n",
       "      <td>Qwen2.5-14B-Instruct</td>\n",
       "      <td>1</td>\n",
       "      <td>64000401</td>\n",
       "      <td>True</td>\n",
       "      <td>SSSV scale</td>\n",
       "      <td>0.294215</td>\n",
       "      <td>0.705785</td>\n",
       "      <td>0.294215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-27.7500</td>\n",
       "      <td>-28.625</td>\n",
       "      <td>Qwen2.5-14B-Instruct</td>\n",
       "      <td>2</td>\n",
       "      <td>64000401</td>\n",
       "      <td>True</td>\n",
       "      <td>SSSV scale</td>\n",
       "      <td>0.705785</td>\n",
       "      <td>0.294215</td>\n",
       "      <td>0.294215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-27.6250</td>\n",
       "      <td>-29.000</td>\n",
       "      <td>Qwen2.5-14B-Instruct</td>\n",
       "      <td>3</td>\n",
       "      <td>64000401</td>\n",
       "      <td>True</td>\n",
       "      <td>SSSV scale</td>\n",
       "      <td>0.798187</td>\n",
       "      <td>0.201813</td>\n",
       "      <td>0.201813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-26.6250</td>\n",
       "      <td>-27.000</td>\n",
       "      <td>Qwen2.5-14B-Instruct</td>\n",
       "      <td>4</td>\n",
       "      <td>64000401</td>\n",
       "      <td>True</td>\n",
       "      <td>SSSV scale</td>\n",
       "      <td>0.592667</td>\n",
       "      <td>0.407333</td>\n",
       "      <td>0.592667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-30.0000</td>\n",
       "      <td>-30.875</td>\n",
       "      <td>Qwen2.5-14B-Instruct</td>\n",
       "      <td>5</td>\n",
       "      <td>64000401</td>\n",
       "      <td>True</td>\n",
       "      <td>SSSV scale</td>\n",
       "      <td>0.705785</td>\n",
       "      <td>0.294215</td>\n",
       "      <td>0.705785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2776555</th>\n",
       "      <td>2</td>\n",
       "      <td>-15.6875</td>\n",
       "      <td>-16.375</td>\n",
       "      <td>Apertus-70B-Instruct-2509</td>\n",
       "      <td>36</td>\n",
       "      <td>68051401</td>\n",
       "      <td>True</td>\n",
       "      <td>SSSV scale</td>\n",
       "      <td>0.665411</td>\n",
       "      <td>0.334589</td>\n",
       "      <td>0.334589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2776556</th>\n",
       "      <td>1</td>\n",
       "      <td>-16.3750</td>\n",
       "      <td>-15.750</td>\n",
       "      <td>Apertus-70B-Instruct-2509</td>\n",
       "      <td>37</td>\n",
       "      <td>68051401</td>\n",
       "      <td>True</td>\n",
       "      <td>SSSV scale</td>\n",
       "      <td>0.348645</td>\n",
       "      <td>0.651355</td>\n",
       "      <td>0.348645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2776557</th>\n",
       "      <td>1</td>\n",
       "      <td>-15.3125</td>\n",
       "      <td>-14.625</td>\n",
       "      <td>Apertus-70B-Instruct-2509</td>\n",
       "      <td>38</td>\n",
       "      <td>68051401</td>\n",
       "      <td>True</td>\n",
       "      <td>SSSV scale</td>\n",
       "      <td>0.334589</td>\n",
       "      <td>0.665411</td>\n",
       "      <td>0.334589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2776558</th>\n",
       "      <td>2</td>\n",
       "      <td>-16.6250</td>\n",
       "      <td>-17.250</td>\n",
       "      <td>Apertus-70B-Instruct-2509</td>\n",
       "      <td>39</td>\n",
       "      <td>68051401</td>\n",
       "      <td>True</td>\n",
       "      <td>SSSV scale</td>\n",
       "      <td>0.651355</td>\n",
       "      <td>0.348645</td>\n",
       "      <td>0.348645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2776559</th>\n",
       "      <td>2</td>\n",
       "      <td>-16.0000</td>\n",
       "      <td>-16.500</td>\n",
       "      <td>Apertus-70B-Instruct-2509</td>\n",
       "      <td>40</td>\n",
       "      <td>68051401</td>\n",
       "      <td>True</td>\n",
       "      <td>SSSV scale</td>\n",
       "      <td>0.622459</td>\n",
       "      <td>0.377541</td>\n",
       "      <td>0.377541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2776560 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         human_number        1       2                      model  item  \\\n",
       "0                   1 -31.3750 -30.500       Qwen2.5-14B-Instruct     1   \n",
       "1                   2 -27.7500 -28.625       Qwen2.5-14B-Instruct     2   \n",
       "2                   2 -27.6250 -29.000       Qwen2.5-14B-Instruct     3   \n",
       "3                   1 -26.6250 -27.000       Qwen2.5-14B-Instruct     4   \n",
       "4                   1 -30.0000 -30.875       Qwen2.5-14B-Instruct     5   \n",
       "...               ...      ...     ...                        ...   ...   \n",
       "2776555             2 -15.6875 -16.375  Apertus-70B-Instruct-2509    36   \n",
       "2776556             1 -16.3750 -15.750  Apertus-70B-Instruct-2509    37   \n",
       "2776557             1 -15.3125 -14.625  Apertus-70B-Instruct-2509    38   \n",
       "2776558             2 -16.6250 -17.250  Apertus-70B-Instruct-2509    39   \n",
       "2776559             2 -16.0000 -16.500  Apertus-70B-Instruct-2509    40   \n",
       "\n",
       "         participant  flipped  experiment    prob_1    prob_2  prob_pred  \n",
       "0           64000401     True  SSSV scale  0.294215  0.705785   0.294215  \n",
       "1           64000401     True  SSSV scale  0.705785  0.294215   0.294215  \n",
       "2           64000401     True  SSSV scale  0.798187  0.201813   0.201813  \n",
       "3           64000401     True  SSSV scale  0.592667  0.407333   0.592667  \n",
       "4           64000401     True  SSSV scale  0.705785  0.294215   0.705785  \n",
       "...              ...      ...         ...       ...       ...        ...  \n",
       "2776555     68051401     True  SSSV scale  0.665411  0.334589   0.334589  \n",
       "2776556     68051401     True  SSSV scale  0.348645  0.651355   0.348645  \n",
       "2776557     68051401     True  SSSV scale  0.334589  0.665411   0.334589  \n",
       "2776558     68051401     True  SSSV scale  0.651355  0.348645   0.348645  \n",
       "2776559     68051401     True  SSSV scale  0.622459  0.377541   0.377541  \n",
       "\n",
       "[2776560 rows x 11 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SSSV_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd874639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flip back human answers where they were flipped\n",
    "mask = (SSSV_data[\"flipped\"] == True) \n",
    "SSSV_data.loc[mask, \"human_number\"] = 3 - SSSV_data.loc[mask, \"human_number\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46968f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse human answers (again) where the items where reversed phrased\n",
    "\n",
    "# add whether item was reverse coded\n",
    "reverse_coded = {\n",
    "     1: True, 2: False, 3: True, 4: False, 5: True, 6: True, 7: False, 8: True, 9: True, 10: False, \n",
    "     11: False, 12: False, 13: False, 14: True, 15: False, 16: True, 17: True, 18: True, 19: False, 20: False,\n",
    "     21: False, 22: True, 23: True, 24: True, 25: False, 26: False, 27: False, 28: True, 29: True, 30: False,\n",
    "     31: False, 32: True, 33: False, 34: True, 35: False, 36: True, 37: False, 38: False, 39: True, 40: False\n",
    "\n",
    "}\n",
    "\n",
    "# Apply mapping row-wise based on item number\n",
    "SSSV_data[\"reverse_coded\"] = SSSV_data[\"item\"].map(reverse_coded)\n",
    "\n",
    "# flip back answers that where reverse coded\n",
    "mask = (SSSV_data[\"reverse_coded\"] == True)\n",
    "SSSV_data.loc[mask, \"human_number\"] = 3 - SSSV_data.loc[mask, \"human_number\"]\n",
    "# drop reverse-coded column (not needed in final data)\n",
    "model_item_scores_SSSV = SSSV_data.drop(columns=[\"reverse_coded\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4ec1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g6/6tx7kz_51_92m63qx0q2c2lw0000gn/T/ipykernel_12395/2881964319.py:32: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  score = grouped.apply(\n"
     ]
    }
   ],
   "source": [
    "# produce df with one value per model per item \n",
    "model_item_scores_SSSV = get_LLM_value_per_item(SSSV_data)\n",
    "model_item_scores_SSSV_top_n = get_LLM_value_per_item_top_n(SSSV_data)\n",
    "\n",
    "# Merge them on the grouping keys\n",
    "model_item_scores_SSSV = model_item_scores_SSSV.merge(\n",
    "    model_item_scores_SSSV_top_n,\n",
    "    on=[\"experiment\", \"model\", \"item\"],\n",
    "    how=\"inner\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7bb971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding task specific categories to save in all data\n",
    "\n",
    "# add item categories\n",
    "item_to_category = {\n",
    "     3: \"SStas\", 11: \"SStas\", 16: \"SStas\", 17: \"SStas\", 20: \"SStas\", 21: \"SStas\", 23: \"SStas\", 28: \"SStas\", 38: \"SStas\", 40: \"SStas\",\n",
    "     4: \"SSexp\", 6: \"SSexp\", 9: \"SSexp\", 10: \"SSexp\", 14: \"SSexp\", 18: \"SSexp\", 19: \"SSexp\", 22: \"SSexp\", 26: \"SSexp\", 37: \"SSexp\",\n",
    "     1: \"SSdis\", 12: \"SSdis\", 13: \"SSdis\", 25: \"SSdis\", 29: \"SSdis\", 30: \"SSdis\", 32: \"SSdis\", 33: \"SSdis\", 35: \"SSdis\", 36: \"SSdis\",\n",
    "     2: \"SSbor\", 5: \"SSbor\", 7: \"SSbor\", 8: \"SSbor\", 15: \"SSbor\", 24: \"SSbor\", 27: \"SSbor\", 31: \"SSbor\", 34: \"SSbor\", 39: \"SSbor\"\n",
    "}\n",
    "\n",
    "model_item_scores_SSSV[\"category\"] = model_item_scores_SSSV[\"item\"].map(item_to_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c256df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dfs\n",
    "all_data = pd.concat([all_data, model_item_scores_SSSV], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b140480",
   "metadata": {},
   "source": [
    "# Saving new processed dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a81d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "all_data.to_csv('processed_data/items_per_LLM.csv', index=False)\n",
    "#all_data.to_csv(\"processed_data/items_per_LLM_random_simulation.csv\", index=False)\n",
    "#all_data.to_csv(\"processed_data/items_per_LLM_semi_random_simulation.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7afd59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment</th>\n",
       "      <th>model</th>\n",
       "      <th>item</th>\n",
       "      <th>score</th>\n",
       "      <th>score_top_n</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AUDIT scale</td>\n",
       "      <td>Apertus-70B-Instruct-2509</td>\n",
       "      <td>1</td>\n",
       "      <td>0.927946</td>\n",
       "      <td>0.431521</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AUDIT scale</td>\n",
       "      <td>Apertus-70B-Instruct-2509</td>\n",
       "      <td>2</td>\n",
       "      <td>2.674954</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AUDIT scale</td>\n",
       "      <td>Apertus-70B-Instruct-2509</td>\n",
       "      <td>3</td>\n",
       "      <td>0.656189</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AUDIT scale</td>\n",
       "      <td>Apertus-70B-Instruct-2509</td>\n",
       "      <td>4</td>\n",
       "      <td>1.163460</td>\n",
       "      <td>0.984894</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AUDIT scale</td>\n",
       "      <td>Apertus-70B-Instruct-2509</td>\n",
       "      <td>5</td>\n",
       "      <td>0.134567</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11817</th>\n",
       "      <td>SSSV scale</td>\n",
       "      <td>zephyr-7b-beta</td>\n",
       "      <td>36</td>\n",
       "      <td>1.422210</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>SSdis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>SSSV scale</td>\n",
       "      <td>zephyr-7b-beta</td>\n",
       "      <td>37</td>\n",
       "      <td>1.791699</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>SSexp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>SSSV scale</td>\n",
       "      <td>zephyr-7b-beta</td>\n",
       "      <td>38</td>\n",
       "      <td>1.661893</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>SStas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>SSSV scale</td>\n",
       "      <td>zephyr-7b-beta</td>\n",
       "      <td>39</td>\n",
       "      <td>1.570623</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>SSbor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>SSSV scale</td>\n",
       "      <td>zephyr-7b-beta</td>\n",
       "      <td>40</td>\n",
       "      <td>1.577203</td>\n",
       "      <td>1.590000</td>\n",
       "      <td>SStas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11822 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        experiment                      model  item     score  score_top_n  \\\n",
       "0      AUDIT scale  Apertus-70B-Instruct-2509     1  0.927946     0.431521   \n",
       "1      AUDIT scale  Apertus-70B-Instruct-2509     2  2.674954     3.000000   \n",
       "2      AUDIT scale  Apertus-70B-Instruct-2509     3  0.656189     0.000000   \n",
       "3      AUDIT scale  Apertus-70B-Instruct-2509     4  1.163460     0.984894   \n",
       "4      AUDIT scale  Apertus-70B-Instruct-2509     5  0.134567     0.000000   \n",
       "...            ...                        ...   ...       ...          ...   \n",
       "11817   SSSV scale             zephyr-7b-beta    36  1.422210     2.000000   \n",
       "11818   SSSV scale             zephyr-7b-beta    37  1.791699     1.000000   \n",
       "11819   SSSV scale             zephyr-7b-beta    38  1.661893     1.000000   \n",
       "11820   SSSV scale             zephyr-7b-beta    39  1.570623     2.000000   \n",
       "11821   SSSV scale             zephyr-7b-beta    40  1.577203     1.590000   \n",
       "\n",
       "      category  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  \n",
       "...        ...  \n",
       "11817    SSdis  \n",
       "11818    SSexp  \n",
       "11819    SStas  \n",
       "11820    SSbor  \n",
       "11821    SStas  \n",
       "\n",
       "[11822 rows x 6 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
